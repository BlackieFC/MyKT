{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cora.cites 和 cora.content 文件已保存。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904962/3682346452.py:71: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  adj_matrix = nx.adjacency_matrix(G, nodelist=nodes).todense()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import toml\n",
    "\n",
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "version = \"v2_5\"\n",
    "\n",
    "data_path = config[f'data_path_{version}']\n",
    "jsonl_path = config[f'jsonl_file_{version}']\n",
    "content_data = config[f'content_{version}']\n",
    "cites_data = config[f'cites_{version}']\n",
    "\n",
    "jsonl_file = os.path.join(data_path, jsonl_path)\n",
    "content_file = os.path.join(data_path, content_data)\n",
    "cites_file = os.path.join(data_path, cites_data)\n",
    "\n",
    "# 读取jsonl文件并提取theme_1和topic的关系\n",
    "edges = []\n",
    "themes = set()\n",
    "topics = set()\n",
    "\n",
    "with open(jsonl_file, 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        theme_1 = f\"theme_{data['theme_1']}\"  # 添加前缀 \"theme_\"\n",
    "        topic_list = data['topic_map'] \n",
    "        \n",
    "        # 如果 topic 是字符串而非列表，转换为单元素列表\n",
    "        if isinstance(topic_list, str):\n",
    "            topic_list = [topic_list]\n",
    "        \n",
    "        for topic in topic_list:\n",
    "            topic = f\"topic_{topic}\"  # 添加前缀 \"topic_\"\n",
    "            edges.append((theme_1, topic))\n",
    "            themes.add(theme_1)\n",
    "            topics.add(topic)\n",
    "\n",
    "# 创建一个带权重的图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 添加边并处理重复边增加权重\n",
    "for edge in edges:\n",
    "    if G.has_edge(*edge):\n",
    "        G[edge[0]][edge[1]]['weight'] += 1  # 如果边已存在，增加权重\n",
    "    else:\n",
    "        G.add_edge(*edge, weight=1)  # 如果边不存在，设置初始权重为1\n",
    "\n",
    "# 在添加边之后,图构建完成之后\n",
    "for u, v, d in G.edges(data=True):\n",
    "    G[u][v]['weight'] = 1 + np.log1p(G[u][v]['weight'])  # log1p(x) = log(1+x)\n",
    "\n",
    "# 然后再进行归一化\n",
    "max_weight = max(edge[2]['weight'] for edge in G.edges(data=True))\n",
    "for u, v, d in G.edges(data=True):\n",
    "    G[u][v]['weight'] /= max_weight\n",
    "\n",
    "# 为每个节点添加标签\n",
    "for theme in themes:\n",
    "    G.nodes[theme]['label'] = 'theme'  # 给 theme 节点添加标签\n",
    "\n",
    "for topic in topics:\n",
    "    G.nodes[topic]['label'] = 'topic'  # 给 topic 节点添加标签\n",
    "\n",
    "# 构建节点特征矩阵和邻接矩阵\n",
    "nodes = list(G.nodes)\n",
    "adj_matrix = nx.adjacency_matrix(G, nodelist=nodes).todense()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(nodes)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "feature_matrix = np.eye(num_classes)[labels]\n",
    "\n",
    "# 更新 cites_lines\n",
    "cites_lines = []\n",
    "for edge in G.edges(data=True):\n",
    "    cites_line = f\"{edge[0]}\\t{edge[1]}\\t{edge[2]['weight']:.4f}\"\n",
    "    cites_lines.append(cites_line)\n",
    "\n",
    "# 保存cora.cites文件\n",
    "with open(cites_file, \"w\") as f:\n",
    "    for line in cites_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "content_lines = []\n",
    "node_ids = list(G.nodes)\n",
    "for node_id in node_ids:\n",
    "    attributes = [1 if G.has_edge(node_id, other_id) or G.has_edge(other_id, node_id) else 0 for other_id in node_ids]\n",
    "    data = G.nodes[node_id]\n",
    "    label = data['label']\n",
    "    content_line = f\"{node_id}\\t{' '.join(map(str, attributes))}\\t{label}\"\n",
    "    content_lines.append(content_line)\n",
    "\n",
    "# 保存cora.content文件\n",
    "with open(content_file, \"w\") as f:\n",
    "    for line in content_lines:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(\"cora.cites 和 cora.content 文件已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/AuroraX-00/share_v3/caoying/anaconda3/envs/recom/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Computing transition probabilities: 100%|██████████| 242/242 [00:01<00:00, 130.31it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 125/125 [00:01<00:00, 65.97it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 125/125 [00:01<00:00, 65.68it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 125/125 [00:01<00:00, 65.21it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 125/125 [00:01<00:00, 66.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2vec_embeddings: (242, 128)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import toml\n",
    "\n",
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "version = \"v2_5\"\n",
    "\n",
    "data_path = config[f'data_path_{version}']\n",
    "jsonl_path = config[f'jsonl_file_{version}']\n",
    "content_data = config[f'content_{version}']\n",
    "cites_data = config[f'cites_{version}']\n",
    "\n",
    "model_save_path = config['model_save_path']\n",
    "png_path = config['png_path']\n",
    "dimensions = int(config['dimensions'])\n",
    "theme_emb_path = config['theme_emb_path']\n",
    "topic_emb_path = config['topic_emb_path']\n",
    "\n",
    "def seed_everything(seed=2023):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "seed_everything()\n",
    "def load_cora_data(data_path=data_path):\n",
    "    content_df = pd.read_csv(os.path.join(data_path, content_data), delimiter=\"\\t\", header=None)\n",
    "    content_df.set_index(0, inplace=True)\n",
    "    index = content_df.index.tolist()\n",
    "\n",
    "    features = np.array([list(map(float, row[0].split())) for row in content_df.iloc[:, :-1].values])\n",
    "    features = sp.csr_matrix(features, dtype=np.float32)\n",
    "\n",
    "    labels = content_df.values[:, -1]\n",
    "    class_encoder = LabelEncoder()\n",
    "    labels = class_encoder.fit_transform(labels)\n",
    "\n",
    "    cites_df = pd.read_csv(os.path.join(data_path, cites_data), delimiter=\"\\t\", header=None)\n",
    "    cites_df[0] = cites_df[0].astype(str)\n",
    "    cites_df[1] = cites_df[1].astype(str)\n",
    "    cites_df[2] = cites_df[2].astype(float)  \n",
    "    \n",
    "    cites = [tuple(x) for x in cites_df.values]\n",
    "    edges = [(index.index(cite[0]), index.index(cite[1])) for cite in cites]\n",
    "    weights = cites_df[2].values  # 提取权重\n",
    "\n",
    "    edges = np.array(edges).T\n",
    "    data = Data(x=torch.from_numpy(np.array(features.todense())),\n",
    "                edge_index=torch.LongTensor(edges),\n",
    "                edge_attr=torch.FloatTensor(weights).unsqueeze(1),  # 加入边的权重\n",
    "                y=torch.from_numpy(labels))\n",
    "\n",
    "    idx_train = range(242)\n",
    "    idx_val = range(242)\n",
    "    idx_test = range(242)\n",
    "\n",
    "    def index_to_mask(index, size):\n",
    "        mask = np.zeros(size, dtype=bool)\n",
    "        mask[index] = True\n",
    "        return mask\n",
    "\n",
    "    data.train_mask = index_to_mask(idx_train, size=labels.shape[0])\n",
    "    data.val_mask = index_to_mask(idx_val, size=labels.shape[0])\n",
    "    data.test_mask = index_to_mask(idx_test, size=labels.shape[0])\n",
    "\n",
    "    def to_networkx(data):\n",
    "        edge_index = data.edge_index.to(torch.device('cpu')).numpy()\n",
    "        edge_weights = data.edge_attr.to(torch.device('cpu')).numpy()\n",
    "        G = nx.Graph()\n",
    "        for i, (src, tar) in enumerate(edge_index.T):\n",
    "            G.add_edge(index[src], index[tar] ,weight=edge_weights[i][0])  # 添加权重\n",
    "        return G\n",
    "\n",
    "    networkx_data = to_networkx(data)\n",
    "\n",
    "    return data, networkx_data, index\n",
    "\n",
    "pyg_data, networkx_data, node_index = load_cora_data()\n",
    "\n",
    "def Node2Vec_run(networkx_data, dimensions=128, walk_length=30, num_walks=200, model_save_path=\"node2vec2.model\"):\n",
    "    p = 0.5\n",
    "    #q = 2  \n",
    "    q = 0.5\n",
    "\n",
    "    node2vec = Node2Vec(\n",
    "    networkx_data, \n",
    "    dimensions=dimensions, \n",
    "    walk_length=walk_length, \n",
    "    num_walks=num_walks, \n",
    "    workers=4,\n",
    "    p=p,  # 添加 p 参数\n",
    "    q=q   # 添加 q 参数\n",
    ")\n",
    "    model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "    nodes = model.wv.index_to_key\n",
    "    embeddings = model.wv[nodes]\n",
    "    \n",
    "    # 保存模型\n",
    "    model.save(model_save_path)\n",
    "    \n",
    "    return model, nodes, embeddings\n",
    "\n",
    "def plot_embeddings(embeddings, labels, output_path):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap=\"jet\", alpha=0.6)\n",
    "    legend1 = plt.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_embeddings2(embeddings, labels, output_path, node_index):\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=labels, cmap=\"jet\", alpha=0.6)\n",
    "    \n",
    "    # 添加颜色条\n",
    "    plt.colorbar(scatter)\n",
    "\n",
    "    # 添加一些节点的标签\n",
    "    num_labels = min(20, len(node_index))  # 限制标签数量以避免过度拥挤\n",
    "    step = len(node_index) // num_labels\n",
    "    for i in range(0, len(node_index), step):\n",
    "        plt.annotate(node_index[i], (reduced_embeddings[i, 0], reduced_embeddings[i, 1]), fontsize=8)\n",
    "\n",
    "    plt.title(\"Node2Vec Embeddings Visualization\")\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def save_embeddings(embeddings, node_index):\n",
    "    if len(embeddings) != len(node_index):\n",
    "        raise ValueError(\"The length of embeddings and node_index must match.\")\n",
    "\n",
    "    embeddings_df = pd.DataFrame(embeddings)\n",
    "    embeddings_df['index'] = node_index\n",
    "    # 将 index 列移动到第一列\n",
    "    embeddings_df = embeddings_df[['index'] + [col for col in embeddings_df.columns if col != 'index']]\n",
    "    # 分离开头是'theme_'的数据和开头是'topic_'的数据\n",
    "    theme_df = embeddings_df[embeddings_df['index'].str.startswith('theme_')]\n",
    "    topic_df = embeddings_df[embeddings_df['index'].str.startswith('topic_')]\n",
    "    theme_df.to_csv(os.path.join(data_path, theme_emb_path), index=False)\n",
    "    topic_df.to_csv(os.path.join(data_path, topic_emb_path), index=False)\n",
    "\n",
    "_, _, node2vec_embeddings = Node2Vec_run(networkx_data, dimensions=dimensions, walk_length=10, num_walks=500, model_save_path=os.path.join(data_path, model_save_path))\n",
    "print(\"node2vec_embeddings:\", np.array(node2vec_embeddings).shape)\n",
    "plot_embeddings2(node2vec_embeddings, pyg_data.y.numpy(), os.path.join(data_path, png_path))\n",
    "save_embeddings(node2vec_embeddings, node_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分别保存teme和topic的相似度矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All nodes in the model:\n",
      "theme_日常生活\n",
      "theme_旅游与交通\n",
      "theme_学习与教育\n",
      "theme_兴趣与爱好\n",
      "theme_学校\n",
      "theme_家庭\n",
      "theme_文娱与体育\n",
      "theme_个人信息\n",
      "theme_购物\n",
      "theme_计划与安排\n",
      "theme_人际交往\n",
      "theme_地点\n",
      "theme_卫生与健康\n",
      "theme_节假日活动\n",
      "theme_工作\n",
      "theme_媒体与影视\n",
      "theme_动物\n",
      "theme_人物描写\n",
      "theme_物品\n",
      "theme_饮食\n",
      "theme_安全与救护\n",
      "theme_故事\n",
      "theme_世界与环境\n",
      "theme_求助\n",
      "theme_天气\n",
      "theme_文学与艺术\n",
      "theme_服务\n",
      "theme_情感与情绪\n",
      "theme_服装与颜色\n",
      "theme_历史与社会\n",
      "theme_经历\n",
      "theme_数字与时间\n",
      "theme_知识与科普\n",
      "theme_通讯\n",
      "topic_经历\n",
      "topic_家庭\n",
      "topic_活动\n",
      "topic_爱好\n",
      "topic_计划\n",
      "topic_社会\n",
      "topic_个人\n",
      "topic_学习\n",
      "topic_交流\n",
      "topic_时间\n",
      "topic_旅游\n",
      "topic_体育\n",
      "topic_情感\n",
      "topic_通讯\n",
      "topic_购物\n",
      "topic_分析\n",
      "topic_建议\n",
      "topic_态度\n",
      "topic_教育\n",
      "topic_地理\n",
      "topic_生活\n",
      "topic_互动\n",
      "topic_工作\n",
      "topic_美食\n",
      "topic_交通\n",
      "topic_社交\n",
      "topic_物品\n",
      "topic_医疗\n",
      "topic_总结\n",
      "topic_信息\n",
      "topic_文化\n",
      "topic_天气\n",
      "topic_动物\n",
      "topic_决策\n",
      "topic_自然\n",
      "topic_询问\n",
      "topic_环境\n",
      "topic_健康\n",
      "topic_指引\n",
      "topic_日常\n",
      "topic_建筑\n",
      "topic_承诺\n",
      "topic_案件\n",
      "topic_习惯\n",
      "topic_人物\n",
      "topic_地点\n",
      "topic_礼物\n",
      "topic_前景\n",
      "topic_心理\n",
      "topic_媒体\n",
      "topic_选择\n",
      "topic_行为\n",
      "topic_语言\n",
      "topic_友谊\n",
      "topic_科技\n",
      "topic_服装\n",
      "topic_饮食\n",
      "topic_图书\n",
      "topic_娱乐\n",
      "topic_反馈\n",
      "topic_特殊情况\n",
      "topic_艺术\n",
      "topic_电影\n",
      "topic_技能\n",
      "topic_电话\n",
      "topic_节日\n",
      "topic_剧情\n",
      "topic_摄影\n",
      "topic_休闲\n",
      "topic_历史\n",
      "topic_安慰\n",
      "topic_失物\n",
      "topic_儿童\n",
      "topic_音乐\n",
      "topic_距离\n",
      "topic_书籍\n",
      "topic_危险\n",
      "topic_国家\n",
      "topic_经济\n",
      "topic_合作\n",
      "topic_人际\n",
      "topic_工具\n",
      "topic_咨询\n",
      "topic_目标\n",
      "topic_辨识\n",
      "topic_交易\n",
      "topic_告别\n",
      "topic_礼仪\n",
      "topic_纪录\n",
      "topic_梦想\n",
      "topic_安全\n",
      "topic_社区\n",
      "topic_突发事件\n",
      "topic_身份\n",
      "topic_评价\n",
      "topic_互联网\n",
      "topic_计算机\n",
      "topic_运动\n",
      "topic_挑战\n",
      "topic_故事\n",
      "topic_讨论\n",
      "topic_学科\n",
      "topic_帮助\n",
      "topic_校园\n",
      "topic_服务\n",
      "topic_科学\n",
      "topic_分类\n",
      "topic_成就\n",
      "topic_事务\n",
      "topic_商业\n",
      "topic_设备\n",
      "topic_宠物\n",
      "topic_压力\n",
      "topic_问题\n",
      "topic_原因\n",
      "topic_价格\n",
      "topic_任务\n",
      "topic_学校\n",
      "topic_食品\n",
      "topic_智慧\n",
      "topic_错误\n",
      "topic_家长\n",
      "topic_友情\n",
      "topic_反应\n",
      "topic_农业\n",
      "topic_比较\n",
      "topic_奖项\n",
      "topic_质量\n",
      "topic_表达\n",
      "topic_植物\n",
      "topic_技术\n",
      "topic_描述\n",
      "topic_地址\n",
      "topic_创作\n",
      "topic_品质\n",
      "topic_位置\n",
      "topic_商品\n",
      "topic_家居\n",
      "topic_公园\n",
      "topic_人文\n",
      "topic_资源\n",
      "topic_金融\n",
      "topic_关怀\n",
      "topic_推理\n",
      "topic_听力\n",
      "topic_团队\n",
      "topic_特点\n",
      "topic_概念\n",
      "topic_称赞\n",
      "topic_标志\n",
      "topic_习俗\n",
      "topic_玩具\n",
      "topic_寻求\n",
      "topic_竞赛\n",
      "topic_规则\n",
      "topic_贡献\n",
      "topic_驾驶\n",
      "topic_海洋\n",
      "topic_便利性\n",
      "topic_观念\n",
      "topic_英雄\n",
      "topic_管理\n",
      "topic_城市\n",
      "topic_关系\n",
      "topic_广告\n",
      "topic_多样性\n",
      "topic_招聘\n",
      "topic_去向\n",
      "topic_住宿\n",
      "topic_努力\n",
      "topic_现代化\n",
      "topic_功能\n",
      "topic_博物馆\n",
      "topic_表演\n",
      "topic_来源\n",
      "topic_困惑\n",
      "topic_航空\n",
      "topic_生存\n",
      "topic_榜样\n",
      "topic_认知\n",
      "topic_度假\n",
      "topic_需求\n",
      "topic_维修\n",
      "topic_邮政\n",
      "topic_检查\n",
      "topic_重量\n",
      "topic_能力\n",
      "topic_许可\n",
      "topic_景观\n",
      "topic_灵感\n",
      "topic_权力\n",
      "topic_舞蹈\n",
      "topic_写作\n",
      "topic_目击\n",
      "topic_状态\n",
      "topic_政治\n",
      "topic_模型\n",
      "topic_文物\n",
      "topic_药品\n",
      "topic_材料\n",
      "topic_方法\n",
      "topic_残障\n",
      "topic_烹饪\n",
      "topic_问答\n",
      "topic_事实\n",
      "topic_奇幻\n",
      "topic_裁判\n",
      "topic_食物\n",
      "Topic results have been saved to /mnt/new_pfs/liming_team/auroraX/caoying/Graph-Networks/topic_data/v2.5/topic_sim.xlsx.\n",
      "Theme results have been saved to /mnt/new_pfs/liming_team/auroraX/caoying/Graph-Networks/topic_data/v2.5/theme_sim.xlsx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import toml\n",
    "\n",
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "version = \"v2_5\"\n",
    "\n",
    "data_path = config[f'data_path_{version}']\n",
    "content_data = config[f'content_{version}']\n",
    "cites_data = config[f'cites_{version}']\n",
    "model_save_path = config['model_save_path']\n",
    "theme_sim_path = config['theme_sim_path']\n",
    "topic_sim_path = config['topic_sim_path']\n",
    "\n",
    "\n",
    "class configs():\n",
    "    def __init__(self):\n",
    "        self.data_path = data_path\n",
    "        self.model_path = os.path.join(data_path, model_save_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = configs()\n",
    "\n",
    "def load_cora_data(data_path=data_path):\n",
    "    content_df = pd.read_csv(os.path.join(data_path, content_data), delimiter=\"\\t\", header=None)\n",
    "    content_df.set_index(0, inplace=True)\n",
    "    index = content_df.index.astype(str).tolist()  # 使用字符串索引\n",
    "\n",
    "    original_content = content_df.iloc[:, -1].tolist()\n",
    "\n",
    "    return original_content, index\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = Word2Vec.load(model_path)\n",
    "    \n",
    "    # 打印所有节点名称\n",
    "    all_nodes = model.wv.index_to_key\n",
    "    print(\"All nodes in the model:\")\n",
    "    for node in all_nodes:\n",
    "        print(node)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def find_similar_vectors(model, target_node, node_index, top_k=5):\n",
    "    # 获取目标节点的类别（topic_ 或 theme_）\n",
    "    if target_node.startswith('topic_'):\n",
    "        relevant_nodes = [node for node in model.wv.index_to_key if node.startswith('topic_')]\n",
    "    elif target_node.startswith('theme_'):\n",
    "        relevant_nodes = [node for node in model.wv.index_to_key if node.startswith('theme_')]\n",
    "    else:\n",
    "        raise ValueError(f\"Node '{target_node}' does not start with 'topic_' or 'theme_'\")\n",
    "    \n",
    "    # 获取目标节点的向量\n",
    "    try:\n",
    "        target_vector = model.wv[target_node]\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Key '{target_node}' not present in the model vocabulary\")\n",
    "\n",
    "    # 计算所有相关节点的相似度\n",
    "    relevant_vectors = np.array([model.wv[node] for node in relevant_nodes])\n",
    "    similarities = cosine_similarity([target_vector], relevant_vectors)[0]\n",
    "    \n",
    "    # 设置自身相似度为负无穷大，确保不会被选中\n",
    "    target_index = relevant_nodes.index(target_node)\n",
    "    similarities[target_index] = -np.inf\n",
    "    \n",
    "    # 找到最相似的 top_k 个节点\n",
    "    top_k_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    top_k_similarities = similarities[top_k_indices]\n",
    "    top_k_nodes = [relevant_nodes[i] for i in top_k_indices]\n",
    "\n",
    "    # 将最相似的节点转换为ID\n",
    "    top_k_ids = [node_index.index(node) for node in top_k_nodes]\n",
    "\n",
    "    return top_k_ids, top_k_nodes, top_k_similarities\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    original_content, node_index = load_cora_data(cfg.data_path)\n",
    "    \n",
    "    # 加载已训练的Node2Vec模型\n",
    "    model_save_path = cfg.model_path\n",
    "    model = load_model(model_save_path)\n",
    "    \n",
    "    # 分别存储 topic_ 和 theme_ 的结果\n",
    "    topic_results = []\n",
    "    theme_results = []\n",
    "\n",
    "    for idx, target_node in enumerate(node_index):\n",
    "        if not target_node.startswith('topic_') and not target_node.startswith('theme_'):\n",
    "            continue  # 忽略非 topic_ 和 theme_ 开头的节点\n",
    "        \n",
    "        top_k_ids, top_k_nodes, top_k_similarities = find_similar_vectors(model, target_node, node_index)\n",
    "        \n",
    "        result = {\n",
    "            'id': idx+1,\n",
    "            '节点实际内容': target_node,\n",
    "            '最相似的前5个节点id及相似度': [(top_k_ids[i]+1, top_k_similarities[i]) for i in range(5)]\n",
    "        }\n",
    "        for i in range(5):\n",
    "            result[f'最相似的节点 {i+1}'] = top_k_nodes[i]\n",
    "        \n",
    "        if target_node.startswith('topic_'):\n",
    "            topic_results.append(result)\n",
    "        elif target_node.startswith('theme_'):\n",
    "            theme_results.append(result)\n",
    "\n",
    "    # 将结果保存到 DataFrame 并导出为 Excel 文件\n",
    "    if topic_results:\n",
    "        topic_results_df = pd.DataFrame(topic_results)\n",
    "        topic_output_file_path = os.path.join(data_path, topic_sim_path)\n",
    "        topic_results_df.to_excel(topic_output_file_path, index=False)\n",
    "        print(f\"Topic results have been saved to {topic_output_file_path}.\")\n",
    "\n",
    "    if theme_results:\n",
    "        theme_results_df = pd.DataFrame(theme_results)\n",
    "        theme_output_file_path = os.path.join(data_path, theme_sim_path)\n",
    "        theme_results_df.to_excel(theme_output_file_path, index=False)\n",
    "        print(f\"Theme results have been saved to {theme_output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将 theme 按序拼接到 topic_id文件中\n",
    "## grapth中需要theme节点，所以需要将theme节点id添加到topic_id文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 文件已成功更新并保存为 topics_themes_id.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import toml\n",
    "\n",
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "version = \"v2_5\"\n",
    "\n",
    "data_path = config[f'data_path_{version}']\n",
    "id4topics = config['id4topics']\n",
    "topics_themes_id = config['topics_themes_id']\n",
    "theme_emb_path = config['theme_emb_path']\n",
    "\n",
    "# 获取数据库中 topic话题 与 id的映射表\n",
    "topic_id_file = os.path.join(data_path, id4topics)\n",
    "with open(topic_id_file, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "# 获取当前最大序号\n",
    "max_index = max(data.values())\n",
    "\n",
    "# 读取 theme emb 文件并提取第一列的名字\n",
    "theme_emb_file = os.path.join(data_path, theme_emb_path)\n",
    "names = []\n",
    "with open(theme_emb_file, 'r', encoding='utf-8') as theme_emb:\n",
    "    reader = csv.reader(theme_emb)\n",
    "    names = [row[0] for row in reader if row[0].startswith('theme_')]\n",
    "\n",
    "# 将提取的名字按顺序拼接到 JSON 对象中\n",
    "for name in names:\n",
    "    max_index += 1\n",
    "    data[name] = max_index\n",
    "\n",
    "# 将更新后的 JSON 对象写回文件\n",
    "topics_themes_file = os.path.join(data_path, topics_themes_id)\n",
    "with open(topics_themes_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 文件已成功更新并保存为 topics_themes_id.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将 边文件 cites 中的theme 和 topic转化为id映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import toml\n",
    "\n",
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "version = \"v2_5\"\n",
    "\n",
    "data_path = config[f'data_path_{version}']\n",
    "content_data = config[f'content_{version}']\n",
    "cites_data = config[f'cites_{version}']\n",
    "cites_id_path = config['cites_id']\n",
    "\n",
    "# 需要注意的是topics_themes_id中的 id是从 1 开始的，但是 gnn中要求是从 0 开始的，所以cities映射的时候，要将 id-1 处理\n",
    "# 读取JSON文件并创建名字到ID的映射\n",
    "with open(topics_themes_file, 'r', encoding='utf-8') as json_file:\n",
    "    name_to_id = json.load(json_file)\n",
    "cities_file = os.path.join(data_path, cites_data)\n",
    "# 读取cities文件并进行名字到ID的映射转换\n",
    "with open(cities_file, 'r', encoding='utf-8') as cities_file:\n",
    "    edges = cities_file.readlines()\n",
    "\n",
    "edges_with_id = []\n",
    "for edge in edges:\n",
    "    parts = edge.strip().split('\\t')\n",
    "    if len(parts) < 2 or len(parts) > 3:\n",
    "        print(f\"Warning: Edge '{edge}' is in an unexpected format.\")\n",
    "        continue\n",
    "    node1, node2 = parts[0], parts[1]\n",
    "    weight = parts[2] if len(parts) == 3 else None\n",
    "\n",
    "    if node1.startswith('topic_') :\n",
    "        node1 = node1[len('topic_'):]\n",
    "        \n",
    "    if node2.startswith('topic_') :\n",
    "        node2 = node2[len('topic_'):]\n",
    "    if node1 in name_to_id and node2 in name_to_id:\n",
    "        node1_id = name_to_id.get(node1) - 1\n",
    "        node2_id = name_to_id.get(node2) - 1\n",
    "        if weight is not None:\n",
    "            edges_with_id.append(f\"{node1_id}\\t{node2_id}\\t{weight}\\n\")\n",
    "        else:\n",
    "            edges_with_id.append(f\"{node1_id}\\t{node2_id}\\n\")\n",
    "    else:\n",
    "        print(f\"Warning: One of the nodes '{node1}' or '{node2}' is not found in the JSON mapping.\")\n",
    "\n",
    "# 将转换后的边写入新的文件\n",
    "\n",
    "cites_id_file = os.path.join(data_path, cites_id_path)\n",
    "with open(cites_id_file, 'w', encoding='utf-8') as updated_cities_file:\n",
    "    updated_cities_file.writelines(edges_with_id)\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将两个embedding文件的topic/theme名字改为id，按id序保存在一个文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将两个embedding文件的名字改为id，按序保存\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import toml\n",
    "\n",
    "with open('config.toml', 'r') as f:\n",
    "    config = toml.load(f)\n",
    "    \n",
    "version = \"v2_5\"\n",
    "\n",
    "data_path = config[f'data_path_{version}']\n",
    "theme_emb_path = config[f'theme_emb_path']\n",
    "topic_emb_path = config[f'topic_emb_path']\n",
    "topics_themes_id = config[f'topics_themes_id']\n",
    "topics_themes_id_emb = config[f'topics_themes_id_emb']\n",
    "\n",
    "\n",
    "theme_emb_file = os.path.join(data_path, theme_emb_path)\n",
    "topic_emb_file = os.path.join(data_path, topic_emb_path)\n",
    "topics_themes_id_file = os.path.join(data_path, topics_themes_id)\n",
    "topics_themes_id_emb_file = os.path.join(data_path, topics_themes_id_emb)\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(topics_themes_id_file, 'r', encoding='utf-8') as f:\n",
    "    name_to_id = json.load(f)\n",
    "\n",
    "theme_emb = pd.read_csv(theme_emb_file,header=0)\n",
    "topic_emb = pd.read_csv(topic_emb_file,header=0)\n",
    "\n",
    "theme_emb.iloc[:, 0] = theme_emb.iloc[:, 0].map(name_to_id).apply(lambda x: x - 1 if pd.notna(x) else x)\n",
    "topic_emb.iloc[:, 0] = topic_emb.iloc[:, 0].str.replace('topic_', '')\n",
    "topic_emb.iloc[:, 0] = topic_emb.iloc[:, 0].map(name_to_id).apply(lambda x: x - 1 if pd.notna(x) else x)\n",
    "\n",
    "merged_data = pd.concat([theme_emb, topic_emb])\n",
    "# 按照 ID 排序\n",
    "sorted_data = merged_data.sort_values(by=merged_data.columns[0])\n",
    "# 保存到一个新的文件中\n",
    "sorted_data.to_csv(topics_themes_id_emb_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对齐topic id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 将 top5文件中的topic id 与 处理好的数据集对齐\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# v2.5\n",
    "root_path = '/mnt/new_pfs/liming_team/auroraX/caoying/Graph-Networks/topic_data/v2.5'  # 原始\n",
    "# root_path = './v2.5'                                                                 # 本地克隆\n",
    "tid = os.path.join(root_path, 'PROC_id4topics.json')     # 正确的topic ID字典\n",
    "# tid = os.path.join(root_path, 'topics_themes_id.json')     # 正确的topic & theme ID字典\n",
    "topk = os.path.join(root_path, 'topic_sim.xlsx')           # top5相似文件(topic)\n",
    "# topk = os.path.join(root_path, 'theme_sim.xlsx')         # top5相似文件(theme)\n",
    "out = os.path.join(root_path, 'PROC_topic_sim.xlsx')       # 订正后的文件名(topic)\n",
    "\n",
    "with open(tid, 'r') as file:\n",
    "    tid = json.load(file)  # dict\n",
    "\n",
    "df = pd.read_excel(topk, sheet_name='Sheet1')\n",
    "\n",
    "# 删除前缀\n",
    "df['节点实际内容'] = df['节点实际内容'].str.replace(r'^topic_', '', regex=True)\n",
    "df['最相似的节点 1'] = df['最相似的节点 1'].str.replace(r'^topic_', '', regex=True)\n",
    "df['最相似的节点 2'] = df['最相似的节点 2'].str.replace(r'^topic_', '', regex=True)\n",
    "df['最相似的节点 3'] = df['最相似的节点 3'].str.replace(r'^topic_', '', regex=True)\n",
    "df['最相似的节点 4'] = df['最相似的节点 4'].str.replace(r'^topic_', '', regex=True)\n",
    "df['最相似的节点 5'] = df['最相似的节点 5'].str.replace(r'^topic_', '', regex=True)\n",
    "\n",
    "# check\n",
    "ind_col = df.columns.get_loc('节点实际内容')\n",
    "if set(df.iloc[:len(tid), ind_col].tolist())==set(tid.keys()):\n",
    "    print('check pass')\n",
    "else:\n",
    "    print('Dismatch!')\n",
    "\n",
    "# topk文件中的值与id对应dict\n",
    "dict_topk = dict(zip(df['节点实际内容'], df['id']))\n",
    "\n",
    "# 映射字典：topk中id -> tid\n",
    "dict_proj = {}\n",
    "for key, val in dict_topk.items():\n",
    "    dict_proj[val] = tid[key]  # 将topk中的id（键）映射为数据集中的id（值）\n",
    "\n",
    "# 使用映射字典修正‘最相似的前5个节点id及相似度’列的内容\n",
    "for i in range(df.shape[0]):\n",
    "    # 逐行遍历df\n",
    "    temp = df['最相似的前5个节点id及相似度'].iloc[i]\n",
    "    temp = ast.literal_eval(temp)  # str -> list\n",
    "    temp_fixed = []\n",
    "    for _id, _val in temp:\n",
    "        _id_fixed = dict_proj[_id]\n",
    "        temp_fixed.append((_id_fixed, _val))\n",
    "    # 将修正id后的list->str并重新赋值给单元格\n",
    "    # df['最相似的前5个节点id及相似度'].iloc[i] = str(temp_fixed)\n",
    "    df.loc[i, '最相似的前5个节点id及相似度'] = str(temp_fixed)\n",
    "    df.loc[i, 'id'] = dict_proj[df.loc[i, 'id']]  # 查询新id并修正\n",
    "\n",
    "# 保存\n",
    "df.to_excel(out, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成相似度稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./v2.5/PROC_topic_sim.xlsx\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "0.80375594 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# v2.5\n",
    "# root_path = '/mnt/new_pfs/liming_team/auroraX/caoying/Graph-Networks/topic_data/v2.5'  # 原始\n",
    "root_path = './v2.5'                                                                # 本地克隆\n",
    "\n",
    "print(os.path.join(root_path, 'PROC_topic_sim.xlsx'))\n",
    "# 读取 Excel 文件的指定列\n",
    "df = pd.read_excel(os.path.join(root_path, 'PROC_topic_sim.xlsx'), usecols=['id','最相似的前5个节点id及相似度'])\n",
    "\n",
    "# 使用 ast.literal_eval 转换单元格内容\n",
    "ind_cols = df['id'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "info_raws = df['最相似的前5个节点id及相似度'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# 将整列数据存为一个列表\n",
    "ind_cols = ind_cols.tolist()\n",
    "info_raws = info_raws.tolist()\n",
    "\n",
    "# 生成相似度稀疏矩阵\n",
    "sim_matrix = np.zeros((len(ind_cols)+1, len(ind_cols)+1))  # +1 是因为 id 从 1 开始，(479,479)\n",
    "for ind_col, info_raw in zip(ind_cols, info_raws):\n",
    "    for ind_raw, val in info_raw:\n",
    "        sim_matrix[ind_raw, ind_col] = val\n",
    "\n",
    "# 保存\n",
    "print(sim_matrix)\n",
    "print(np.max(sim_matrix), np.min(sim_matrix))\n",
    "np.save(os.path.join(root_path, 'sim_mat.npy'), sim_matrix)\n",
    "\n",
    "\"\"\"\n",
    "确认一下是否需要对对角线进行赋值（目前不用，对角线等价于直接调整，已经在正常流程中进行）\n",
    "最终输出矩阵的索引0列和行均为0，因为 id 从 1 开始\n",
    "\"\"\"\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
