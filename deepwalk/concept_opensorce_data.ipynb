{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN相关特征的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (不要随意重新运行) 通过 node2vec 获取节点向量（包含调整id0->n_questions）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 241011：注意到除了孤立节点，其他节点embedding的生成具有随机性！！！（一是需要小心覆盖情况；二是检查相似度关系是否稳定！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 94/94 [00:00<00:00, 31614.51it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 50/50 [00:00<00:00, 158.50it/s]\n",
      "Generating walks (CPU: 4): 100%|██████████| 50/50 [00:00<00:00, 157.55it/s]\n",
      "Generating walks (CPU: 2): 100%|██████████| 50/50 [00:00<00:00, 157.35it/s]\n",
      "Generating walks (CPU: 3): 100%|██████████| 50/50 [00:00<00:00, 158.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node embeddings have been saved to node_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from node2vec import Node2Vec\n",
    "import copy\n",
    "\n",
    "# 节点 & 边 原始json文件\n",
    "# concept_map_path = '/mnt/new_pfs/liming_team/auroraX/caoying/github/CRKT/data/DBE_KT22/data/concept_map_vis.json'\n",
    "\n",
    "datasets = 'DBE_KT22' \n",
    "# datasets = 'EdNet'\n",
    "# datasets = 'NIPS34'\n",
    "\n",
    "concept_map_file = f'{datasets}/concept_map_vis.json'      # 包含节点 & 边的信息（这玩意是怎么获得的？）\n",
    "node_embeddings_file = f'{datasets}/node_embeddings.csv'   # 节点embedding（本步骤中生成并保存）\n",
    "concept_sim_file = f'{datasets}/concept_sim.xlsx'          # 节点相似度信息excel（基于节点embedding计算得到）\n",
    "\n",
    "# 读取JSON文件\n",
    "with open(concept_map_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\"\"\"=========================================================================\"\"\"\n",
    "\"\"\"241011: 改写节点和边的信息：将0号节点及其关系复制至93号，同时彻底断开其与其他节点的关系\"\"\"\n",
    "# 修改节点信息\n",
    "max_id = -1\n",
    "for ind, node in enumerate(data['nodes']):\n",
    "    if node[\"id\"] > max_id:\n",
    "        max_id = node[\"id\"]          # 查找最大qid\n",
    "    if node[\"id\"] == 0:\n",
    "        ind_temp = ind\n",
    "        temp = copy.deepcopy(node)   # 深拷贝需要处理的node，并记录其索引\n",
    "# 修改原始node\n",
    "data[\"nodes\"][ind_temp][\"name\"] += '(deprecated)'\n",
    "# 在末尾append复制的新node\n",
    "temp[\"id\"] = max_id + 1\n",
    "data[\"nodes\"].append(temp)\n",
    "\n",
    "# 修改边的信息(将所有与id0节点相关的关系转移给id93)\n",
    "for ind, dict in enumerate(data['links']):\n",
    "    if dict[\"source\"] == 0:\n",
    "        data[\"links\"][ind][\"source\"] = max_id + 1\n",
    "    if dict[\"target\"] == 0:\n",
    "        data[\"links\"][ind][\"target\"] = max_id + 1\n",
    "\"\"\"================================= done! =================================\"\"\"\n",
    "\n",
    "# 创建图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 添加节点\n",
    "for node in data['nodes']:\n",
    "    G.add_node(node['id'], name=node['name'])   # 传参形如 0, \"Set\"\n",
    "\n",
    "# 添加边\n",
    "for link in data['links']:\n",
    "    G.add_edge(link['source'], link['target'])  # 传参形如 0, 1\n",
    "\n",
    "# 创建node2vec模型\n",
    "node2vec = Node2Vec(G, dimensions=128, walk_length=30, num_walks=200, workers=4)\n",
    "\n",
    "# 训练模型\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "\n",
    "# 获取节点的embedding\n",
    "node_embeddings = {node: model.wv[str(node)] for node in G.nodes()}\n",
    "\n",
    "embeddings_dict = {\n",
    "    'index': [],\n",
    "}\n",
    "\n",
    "# 为每个embedding维度创建一个列\n",
    "dimensions = len(next(iter(node_embeddings.values())))  # 128\n",
    "\n",
    "# # check\n",
    "# print(node_embeddings.keys())                     # dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,...\n",
    "# # print(node_embeddings[0])                       # numpy.ndarray, (128,)\n",
    "# print(next(iter(node_embeddings.values())))       # 同上\n",
    "# print(len(next(iter(node_embeddings.values()))))  # 128\n",
    "\n",
    "\n",
    "for i in range(dimensions):\n",
    "    embeddings_dict[f'{i}'] = []\n",
    "    # print(embeddings_dict.keys())  # ['index','0','1',...]\n",
    "\n",
    "# 填充字典\n",
    "for node in G.nodes():\n",
    "    embeddings_dict['index'].append(node)  # index列记录节点ID（int）\n",
    "    embedding = node_embeddings[node]\n",
    "    for i, value in enumerate(embedding):\n",
    "        embeddings_dict[f'{i}'].append(value)  # 按照索引添加至对应的列\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame(embeddings_dict)\n",
    "df.to_csv(node_embeddings_file, index=False)\n",
    "\n",
    "print(\"Node embeddings have been saved to node_embeddings.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (不要随意重新运行) 获取每个节点的5 个相似节点（包含对调整和孤立节点的处理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic results have been saved to DBE_KT22/concept_sim.xlsx.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"假设我们已经有了node_embeddings字典，其中包含了每个节点的embedding向量\"\"\"\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# 1. 准备数据\n",
    "concept_sim_file = f'{datasets}/concept_sim.xlsx'                    # 输出文件名\n",
    "node_ids = list(G.nodes())                                           # 0-93\n",
    "embeddings = np.array([node_embeddings[node] for node in node_ids])  # (94, 128)\n",
    "\n",
    "# 2. 计算余弦相似度矩阵\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# # check\n",
    "# print(similarity_matrix.shape)                                     # (94, 94)\n",
    "# print(np.max(similarity_matrix))\n",
    "# print(np.min(similarity_matrix))\n",
    "# print(similarity_matrix[0,:])\n",
    "# print(similarity_matrix[:,0])\n",
    "\n",
    "\"\"\"=================================================================\"\"\"\n",
    "\"\"\"241011: 确保孤立节点与其他所有节点的相似度为-1(不会被选为其他节点的相似节点)\"\"\"\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    similarity_matrix[i,0] = -1  # 第0列置为-1\n",
    "\"\"\"============================= done! =============================\"\"\"\n",
    "\n",
    "# 3. 找出每个节点最相似的5个节点\n",
    "top_k = 5\n",
    "most_similar = {}\n",
    "\n",
    "for i, node_id in enumerate(node_ids):\n",
    "    # 获取相似度，并排除自身\n",
    "    similarities = [(j, similarity_matrix[i][j]) for j in range(len(node_ids)) if i != j]\n",
    "    # 按相似度降序排序\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)  # 处理后孤立节点相似度为-1，必定排在队尾\n",
    "    # 取前5个最相似的节点\n",
    "    top_similar = similarities[:top_k]\n",
    "\n",
    "    \"\"\"============================================\"\"\"\n",
    "    \"\"\"241011: 改写孤立节点的topk相似节点的相似度，统一为0\"\"\"\n",
    "    if node_id == 0:\n",
    "        top_similar = [(idx, 0) for idx, sim in top_similar]\n",
    "    \"\"\"================== done! ===================\"\"\"\n",
    "    \n",
    "    # 存储结果\n",
    "    most_similar[node_id] = [(node_ids[idx], sim) for idx, sim in top_similar]\n",
    "\n",
    "# 4. 保存结果\n",
    "results = []\n",
    "for node_id, similar_nodes in most_similar.items():\n",
    "    node_name = data['nodes'][node_id]['name']\n",
    "    # print(f\"\\n节点 {node_id} ({node_name}) 的最相似5个节点:\")\n",
    "\n",
    "    # for similar_id, similarity in similar_nodes:\n",
    "    #     similar_name = data['nodes'][similar_id]['name']\n",
    "    #     print(f\"  - 节点 {similar_id} ({similar_name}): 相似度 {similarity:.4f}\")\n",
    "    result={\n",
    "        'id': node_id,\n",
    "        '节点实际内容': node_name,\n",
    "        '最相似的前5个节点id及相似度': [(similar_id, similarity) for similar_id, similarity in similar_nodes]\n",
    "    }\n",
    "    for i,(similar_id, similarity) in enumerate(similar_nodes):\n",
    "        similar_name = data['nodes'][similar_id]['name']\n",
    "        result[f'最相似的节点 {i+1}'] = similar_name\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel(concept_sim_file, index=False)\n",
    "print(f\"Topic results have been saved to {concept_sim_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取DBE_KT22数据集的原始fold数据，整合为txt样本格式（包含调整qid后的pid/q/s信息）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 241017: 新增乱序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成。训练集保存到 train.txt (共1153条记录)，测试集保存到 test.txt (共0条记录)。\n"
     ]
    }
   ],
   "source": [
    "# 获取train.txt和 test.txt（包含调整qid后的pid/q/s信息）—— 暂时将test置空\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "datasets = 'DBE_KT22'\n",
    "# datasets = 'EdNet'\n",
    "# datasets = 'NIPS34'\n",
    "\n",
    "train_file = f'{datasets}/train.txt'\n",
    "test_file = f'{datasets}/test.txt'\n",
    "\n",
    "\n",
    "def process_fold_files(file_paths):\n",
    "    all_data = []\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for item in data:\n",
    "            # user_id = item['user']\n",
    "            questions = item['question']\n",
    "            scores = item['score']\n",
    "            concepts = item['concept']  # 这里concept的id还没进行调整！（查一下具体在哪一步骤调整了？）\n",
    "\n",
    "            \"\"\"=================================== 同步乱序三个list ===================================\"\"\"\n",
    "            packed_list = list(zip(questions, scores, concepts))\n",
    "            random.shuffle(packed_list)\n",
    "            questions, scores, concepts = zip(*packed_list)\n",
    "            questions = list(questions)\n",
    "            scores = list(scores)\n",
    "            concepts = list(concepts)\n",
    "            \"\"\"======================================= done! ========================================\"\"\"\n",
    "            # # check\n",
    "            # print(item)\n",
    "            # raise TypeError\n",
    "\n",
    "            processed_item = [\n",
    "                len(questions),\n",
    "                ','.join(map(str, questions)),\n",
    "                ','.join(['_'.join(map(str, c)) for c in concepts]),\n",
    "                ','.join(map(str, scores))\n",
    "            ]\n",
    "            all_data.append(processed_item)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def write_to_file(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            for field in item:\n",
    "                f.write(f\"{field}\\n\")\n",
    "            # f.write(\"\\n\")  # 在每个用户的数据之间添加一个空行\n",
    "\n",
    "\n",
    "def main():\n",
    "    fold_files = [f'{datasets}/fold{i}.json' for i in range(5)]\n",
    "    all_data = process_fold_files(fold_files)\n",
    "\n",
    "    # 随机打乱数据\n",
    "    # random.shuffle(all_data)\n",
    "    \n",
    "    # 分割为训练集和测试集\n",
    "    split_index = int(len(all_data) * 1.0)  # 暂时将测试集置空，在后面的步骤中再拆分\n",
    "    train_data = all_data[:split_index]\n",
    "    test_data = all_data[split_index:]\n",
    "    # 保存为文本文件\n",
    "    write_to_file(train_data, train_file)\n",
    "    write_to_file(test_data, test_file)\n",
    "    print(f\"处理完成。训练集保存到 train.txt (共{len(train_data)}条记录)，测试集保存到 test.txt (共{len(test_data)}条记录)。\")\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (若不重新生成GCN属性则无需重新运行) 获取图的边连接信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成。链接数据已保存到 DBE_KT22/concept_links.txt\n"
     ]
    }
   ],
   "source": [
    "def process_concept_map(input_file, output_file):\n",
    "    # 读取JSON文件\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 提取links数据\n",
    "    links = data['links']\n",
    "    \n",
    "    # 将数据写入输出文件\n",
    "    with open(output_file, 'w') as f:\n",
    "        for link in links:\n",
    "            source = link['source']\n",
    "            target = link['target']\n",
    "            f.write(f\"{source}\\t{target}\\n\")\n",
    "    \n",
    "    print(f\"处理完成。链接数据已保存到 {output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # datasets = 'DBE_KT22' \n",
    "    # datasets = 'EdNet'\n",
    "    # datasets = 'NIPS34'\n",
    "\n",
    "    concept_map_file = f'{datasets}/concept_map_vis.json'\n",
    "    output_file = f'{datasets}/concept_links.txt'\n",
    "    \n",
    "    process_concept_map(concept_map_file, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同方法生成学生能力并保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）统计历史数据，生成每个学生的能力预测（整合自线上项目的proccess_opensource.ipynb）——后续用于与 策略 和 模型预报 结果进行对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')   # 确保你的图表在后台生成且不显示图形窗口\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "def ensure_directory_exists(path_in):\n",
    "    \"\"\"确保路径存在，否则递归地建立文件夹\"\"\"\n",
    "    if not os.path.exists(path_in):\n",
    "        os.makedirs(path_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 读取txt格式数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4612\n",
      "34\n",
      "\n",
      "<class 'str'>\n",
      "19,0,23,29,2,5,33,24,1,14,15,12,28,31,9,17,3,16,10,11,6,27,7,21,13,26,25,20,22,32,8,18,4,30\n",
      "\n",
      "<class 'str'>\n",
      "6_8_1,0,4_6,7_1,1,3,7_1,4_5_6,0,4_7,5_7,1,6_1,7_1,4,8_1,1,4_6,4_5,5_6,1,4_5_2,6,3_6,4_7,1_2,5_6,5_2,5,7_1,5_6,6_8_1,2,7_1\n",
      "\n",
      "<class 'str'>\n",
      "1,1,1,1,0,1,0,1,1,0,0,1,1,0,1,1,0,1,1,1,1,1,1,1,0,1,0,1,1,0,1,1,1,0\n",
      "\n",
      "<class 'str'>\n",
      "--end of 4 lines--\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "import subprocess\n",
    "\n",
    "def get_line_count(file_in):\n",
    "    result = subprocess.run([\"wc\",\"-l\",file_in], capture_output=True, text=True)\n",
    "    return int(result.stdout.split()[0])\n",
    "\n",
    "def read_n_lines(file_in, n):\n",
    "    with open(file_in, \"r\") as file:\n",
    "        while True:\n",
    "            lines = list(islice(file, n))\n",
    "            if not lines:\n",
    "                break\n",
    "            yield lines\n",
    "\n",
    "# datasets = 'DBE_KT22'\n",
    "# datasets = 'EdNet'\n",
    "# datasets = 'NIPS34'\n",
    "\n",
    "# 调用\n",
    "file_path = f'{datasets}/train.txt'\n",
    "n_lines = 4\n",
    "\n",
    "# 获取行数\n",
    "len_file = get_line_count(file_in=file_path)\n",
    "print(len_file)\n",
    "\n",
    "# test\n",
    "for lines in read_n_lines(file_in=file_path, n=n_lines):\n",
    "    for line in lines:\n",
    "        print(line)  # end=\"\"\n",
    "        print(type(line))\n",
    "    print(\"--end of 4 lines--\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 处理为json接口格式（这一步骤对序列长度进行了筛选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 of 1153\n",
      "200 of 1153\n",
      "300 of 1153\n",
      "400 of 1153\n",
      "500 of 1153\n",
      "600 of 1153\n",
      "700 of 1153\n",
      "800 of 1153\n",
      "900 of 1153\n",
      "1000 of 1153\n",
      "1100 of 1153\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "stu_id = 0\n",
    "for lines in read_n_lines(file_in=file_path, n=n_lines):\n",
    "    stu_id += 1  # 学生id从1起（仅用于查找在txt文件中的位置）\n",
    "    if stu_id % 100 == 0:\n",
    "        print(\"{} of {}\".format(stu_id, len_file//4))\n",
    "\n",
    "    # lines 为长度=4 的列表\n",
    "    n_seq = int(lines[0])\n",
    "    pids = lines[1].strip().split(\",\")  # 得到元素为str的list（且去除多余的\\n）\n",
    "    qs = lines[2].strip().split(\",\")\n",
    "    ss = lines[3].strip().split(\",\")\n",
    "    # qs = ast.literal_eval(\"[\"+ lines[2] +\"]\")  # 错误，会自动将下划线去掉合并为单个int元素\n",
    "\n",
    "\n",
    "    # 判断序列长度是否达标，否则跳过\n",
    "    if n_seq < 100:\n",
    "        continue\n",
    "\n",
    "    # 处理格式，并根据qs生成均匀权重（默认）\n",
    "    pids = list(map(int, pids))\n",
    "    qs = [list(map(int, elem.split(\"_\"))) for elem in qs]  # list of lists(一般长度为1-3)\n",
    "    ss = list(map(int, ss))\n",
    "    w_qs = [[round(1/len(elem), 3)]*len(elem) for elem in qs]\n",
    "    \n",
    "    # 声明接口格式字典，并赋值\n",
    "    data_out = {\n",
    "        \"old_p\": [],\n",
    "        \"new_p\": [],\n",
    "        \"student\": {\n",
    "            \"topic_mastery\": {},\n",
    "            \"id\": stu_id\n",
    "        }\n",
    "    }\n",
    "    for pid, q, w_q, s in zip(pids, qs, w_qs, ss):\n",
    "        # 向\"old_p\"中逐条添加做题记录\n",
    "        record = {          # 每次遍历重新定义，防止指向同一dict\n",
    "            \"pid\": pid,\n",
    "            \"q\": q,\n",
    "            \"q_w\": w_q,\n",
    "            \"w\": [],        # 开源数据集无单词\n",
    "            \"w_w\": [],\n",
    "            \"s\": s,\n",
    "            \"diff\": 1,      # 开源数据集无难度，默认为1\n",
    "            \"session\": 1,\n",
    "            \"corrate\": -1   # 新增正确率项\n",
    "        }\n",
    "        data_out[\"old_p\"].append(record)\n",
    "    \n",
    "    # 汇总为list of dicts\n",
    "    data.append(data_out)\n",
    "\n",
    "# 保存为json文件\n",
    "with open(f'{datasets}/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 根据历史统计，生成学生能力，保存为list of dicts，元素为 学生id：{qid0:qval0, qid1:qval1, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "正确率-能力映射关系参考：\n",
    "def get_solution(coefficient=None, constant=None):\n",
    "    '''\n",
    "    求解线性方程组（diff为离散的二次项）\n",
    "    :param coefficient: np.ndarray, 系数矩阵\n",
    "    :param constant: np.ndarray, 常数项\n",
    "    说明：映射关系如下：\n",
    "        正确率 = A * mean(掌握度)/(max-min) + B * (难度/max)^2 + C\n",
    "    对于KC掌握度取值0-1连续\n",
    "        正确率 = A * mean(掌握度)/(1-0) + B * (难度/1)^2 + C\n",
    "    三种情景：\n",
    "        （1） 最菜的学生（平均掌握度0）做中等难的题（难度1），正确率为 0\n",
    "        （2） 最犇的学生（平均掌握度1）做中等难的题（难度1），正确率为 1\n",
    "        （3） 普通的学生（平均掌握度0.66）做中等难的题（难度1），正确率为 0.7\n",
    "    则有以下三元一次方程组：\n",
    "        （1） 0/1 * A + (-1*(1/1)**2) * B + 1 * C = 0.33      -b+c = 0.25-0.33  => b=0(无效化难度), c=0.25\n",
    "        （2） 1/1 * A + (-1*(1/1)**2) * B + 1 * C = 1            a = 0.66\n",
    "        （3） 2/3 * A + (-1*(1/1)**2) * B + 1 * C = 0.7\n",
    "    即为函数中默认的 coefficient 和 constant 数组取值.\n",
    "    '''\n",
    "    if coefficient is None:\n",
    "        coefficient = np.array([[0/1, -1*(1/1)**2, 1],\n",
    "                                [1/1, -1*(1/1)**2, 1],\n",
    "                                [2/3, -1*(1/1)**2, 1]])\n",
    "    if constant is None:\n",
    "        constant = np.array([0, 1.0, 0.7])\n",
    "    # 求解三元一次方程组\n",
    "    _x, _y, _z = np.linalg.solve(coefficient, constant)\n",
    "    return _x, _y, _z\n",
    "\n",
    "该函数默认情况下无解，因为开源数据集下难度失效了，因此简化为：\n",
    "   0.66 * capability + 0.25 = correct_rate\n",
    "=> (correct_rate - 0.25) * 1.5 = capability\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{datasets}/train.json', \"r\", encoding='UTF-8') as f:\n",
    "    data = json.load(f)  # list of dicts\n",
    "\n",
    "def init_stu_capability(n_q, start=0, padding_value=-1):\n",
    "    keys = range(start, n_q+start)\n",
    "    result = {key: padding_value for key in keys}\n",
    "    return result\n",
    "\n",
    "# # check\n",
    "# print(data[0].keys())\n",
    "# print(data[0][\"student\"])\n",
    "# print(init_stu_capability(93))\n",
    "\n",
    "# 声明必要的参数\n",
    "n_questions = 93\n",
    "n_pid = 212\n",
    "data_out = {}\n",
    "\n",
    "# 遍历每个学生，更新能力表征\n",
    "for stu in data:\n",
    "    stu_cap = init_stu_capability(n_questions)  # 初始化能力字典\n",
    "    stu_id = stu[\"student\"][\"id\"]               # 学生ID\n",
    "\n",
    "    for record in stu[\"old_p\"]:   # list of dicts\n",
    "        for q in record[\"q\"]:     # list of ints\n",
    "            if record[\"s\"] == 1:  # 答对\n",
    "                if isinstance(stu_cap[q], dict):\n",
    "                    stu_cap[q][1] += 1\n",
    "                else:  # 此前未涉及该kp(为填充值-1)，则初始化\n",
    "                    stu_cap[q] = {0:0,1:1}\n",
    "            else:                 # 答错\n",
    "                if isinstance(stu_cap[q], dict):\n",
    "                    stu_cap[q][0] += 1\n",
    "                else:\n",
    "                    stu_cap[q] = {0:1,1:0}\n",
    "            \n",
    "    # 根据统计后的stu_cap计算正确率，再通过公式反演能力\n",
    "    for key, val in stu_cap.items():  # 形如1: {0:20,1:25}\n",
    "        if isinstance(val, dict):\n",
    "            corrate = round(val[1]/(val[0]+val[1]), 2)\n",
    "            capability = (corrate - 0.25) * 1.5      # 公式来源参考下一单元格\n",
    "            capability = max(0, min(capability, 1))  # clip\n",
    "            stu_cap[key] = capability                # update\n",
    "        else:     # 全程未涉及知识点的值仍为-1\n",
    "            pass  # 保持-1不变\n",
    "    \n",
    "    # 将计算得到的能力记录至data_out(以学生ID作为键)\n",
    "    data_out[stu_id] = stu_cap\n",
    "\n",
    "# 将结果保存为json\n",
    "with open(f'{datasets}/stu_cap_stat.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_out, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）通过策略调整，生成每个学生的能力预测（整合自线上项目的concept_predict_linear.ipynb）——后续用于与 统计 和 模型预报 结果进行对比（同时qid的修改对齐也是在这里进行的！）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 导入和自定义策略函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/new_pfs/liming_team/auroraX/songchentao/ketcat/data\n",
      "/mnt/new_pfs/liming_team/auroraX/songchentao/ketcat/data\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')   # 确保你的图表在后台生成且不显示图形窗口\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "# from scipy.sparse import lil_matrix\n",
    "\n",
    "# cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "from config import BASE_DIR\n",
    "cur_dir = BASE_DIR\n",
    "# root_dir = os.path.dirname(os.path.dirname(os.path.dirname(cur_dir)))  # 向上追溯三级\n",
    "root_dir = cur_dir\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "print(cur_dir)   # /mnt/new_pfs/liming_team/auroraX/songchentao/ketcat/data\n",
    "print(root_dir)  # 同上\n",
    "\n",
    "\n",
    "def ensure_directory_exists(path_in):\n",
    "    \"\"\"确保路径存在，否则递归地建立文件夹\"\"\"\n",
    "    if not os.path.exists(path_in):\n",
    "        os.makedirs(path_in)\n",
    "\n",
    "\n",
    "def get_solution(coefficient=None, constant=None):\n",
    "    \"\"\"\n",
    "    求解线性方程组（diff为离散的二次项）\n",
    "    :param coefficient: np.ndarray, 系数矩阵\n",
    "    :param constant: np.ndarray, 常数项\n",
    "    说明：映射关系如下：\n",
    "        正确率 = A * mean(掌握度)/(max-min) + B * (难度/max)^2 + C\n",
    "    对于KC掌握度取值1-4，难度取值1-3：=====================================================================改为0-1连续\n",
    "        正确率 = A * mean(掌握度)/(4-1) + B * (难度/3)^2 + C  == 正确率 = A * mean(掌握度)/(1-0) + B * (难度/3)^2 + C\n",
    "    设定三种情景：\n",
    "        （1）最菜的学生（掌握度1+1+2）做最简单的题（难度1），正确率为 0\n",
    "        （2）最犇的学生（掌握度4+4+4）做最困难的题（难度3），正确率为 1\n",
    "        （3）普通的学生（掌握度2+3+3）做中等难的题（难度2），正确率为 0.7\n",
    "    则有以下三元一次方程组：\n",
    "        （1） 4/9 * A + (-1*(1/3)**2) * B + 1 * C = 0    ======最菜的学生（平均掌握度0）做最简单的题（难度1），正确率为 0\n",
    "        （2）11/9 * A + (-1*(3/3)**2) * B + 1 * C = 1    ======最犇的学生（平均掌握度1）做最困难的题（难度3），正确率为 1\n",
    "        （3） 8/9 * A + (-1*(2/3)**2) * B + 1 * C = 0.7  ======普通的学生（平均掌握度0.66）做中等难的题（难度2），正确率为 0.7\n",
    "    新方程组为：\n",
    "        （1） 0/1 * A + (-1*(1/3)**2) * B + 1 * C = 0\n",
    "        （2） 1/1 * A + (-1*(3/3)**2) * B + 1 * C = 1\n",
    "        （3） 2/3 * A + (-1*(2/3)**2) * B + 1 * C = 0.7\n",
    "    即为函数中默认的 coefficient 和 constant 数组取值.\n",
    "    \"\"\"\n",
    "    if coefficient is None:\n",
    "        # coefficient = np.array([[4/9, -1*(1/3)**2, 1],\n",
    "        #                         [11/9,-1*(3/3)**2, 1],\n",
    "        #                         [8/9, -1*(2/3)**2, 1]])\n",
    "        coefficient = np.array([[0/1, -1*(1/3)**2, 1],\n",
    "                                [1/1, -1*(3/3)**2, 1],\n",
    "                                [2/3, -1*(2/3)**2, 1]])\n",
    "    if constant is None:\n",
    "        constant = np.array([0, 1.0, 0.7])\n",
    "    # 求解三元一次方程组\n",
    "    _x, _y, _z = np.linalg.solve(coefficient, constant)\n",
    "    return _x, _y, _z\n",
    "\n",
    "\n",
    "def plot_pdf(data_in_1, label_1='error', data_in_2=None, label_2=None):\n",
    "    \"\"\"\n",
    "    绘制概率密度函数图\n",
    "    :param data_in: 1d array\n",
    "    \"\"\"\n",
    "    # if data_in is None:\n",
    "    #     # 生成正态分布示例数据\n",
    "    #     np.random.seed(42)\n",
    "    #     data_in = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "    # 使用 seaborn 绘制概率密度函数图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.kdeplot(data_in_1, fill=True, color=\"b\", legend=label_1)\n",
    "    if data_in_2 is not None:\n",
    "        sns.kdeplot(data_in_2, fill=True, color=\"b\", legend=label_1)\n",
    "\n",
    "    # 添加标题和标签\n",
    "    plt.title('Probability Density Function (PDF)')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    # 显示网格\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 保存\n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%Y-%m-%d_%H:%M:%S.%f\")  # 默认格式：YYYY-MM-DD HH:MM:SS.mmmmmm\n",
    "    plt.savefig('pics/pdf_{}.png'.format(now))\n",
    "\n",
    "\n",
    "def plot_histogram(data_in, text=None, file_out=None):\n",
    "    \"\"\"\n",
    "    绘制直方图\n",
    "    :param data_in: 1d array\n",
    "    :param text: str or None\n",
    "    :param file_out: str or None\n",
    "    \"\"\"\n",
    "    # if data_in is None:\n",
    "    #     # 生成正态分布示例数据\n",
    "    #     np.random.seed(42)\n",
    "    #     data_in = np.random.normal(loc=0, scale=1, size=1000)\n",
    "    if text is None:\n",
    "        text = ''\n",
    "\n",
    "    # 使用 seaborn 绘制概率密度函数图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    bins = np.linspace(-1,1,31).tolist()\n",
    "    plt.hist(data_in, bins=bins, color='b', alpha=0.7, edgecolor='black')  # 设定箱子数量为30\n",
    "\n",
    "    plt.xlim([-1,1])\n",
    "    # plt.ylim([0,20])  # 不设置ylim\n",
    "\n",
    "    # 计算文本显示位置\n",
    "    ax = plt.gca()\n",
    "    x_limits = ax.get_xlim()\n",
    "    y_limits = ax.get_ylim()\n",
    "    x_text = x_limits[1] + (x_limits[1] - x_limits[0]) * 0.05  # 超出横坐标最大值的5%\n",
    "    y_text = y_limits[1] + (y_limits[1] - y_limits[0]) * 0.1  # 超出纵坐标最大值的10%\n",
    "    plt.text(x_text, y_text, text, fontsize=12, ha='right', va='top')  # 显示文本\n",
    "\n",
    "    # 添加标题和标签\n",
    "    plt.title('Histogram')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    # 显示网格\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 保存\n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%Y-%m-%d_%H:%M:%S.%f\")  # 默认格式：YYYY-MM-DD HH:MM:SS.mmmmmm\n",
    "    if file_out is None:\n",
    "        file_out = 'pics/histogram_{}.png'.format(now)  # 默认保存文件名\n",
    "    plt.savefig(file_out)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptPredictStrategy:\n",
    "    \"\"\"\n",
    "    self.history 学生在知识点上历史答题记录的统计，可以包括该知识点历史做题数，平均难度，平均正确率\n",
    "    self.knowledge_points 学生的知识点掌握度，由历史答题记录初始化，每次做题后更新\n",
    "    240919更新：\n",
    "        （1）输入输出对齐在线文档《0827-接口文档》，以及engine.py中的调用代码\n",
    "        （2）拆分 topic和 word 相应的方法，适用于两种实际调用场合（topic+单词 / 只需要返回topic维度）\n",
    "        （3）部署时，本class只初始化一次，实例化对象服务于所有用户，因此全局不发生变化的参数在__init__中初始化，对应于config中给定，应当尽可能齐全\n",
    "            目前包含：\n",
    "                知识点相似度矩阵；\n",
    "                和已有参数对齐的各种embedding表大小/起止索引；\n",
    "        （4）self.history：新的框架结构下，调整功能，用于区分topic和word策略：\n",
    "            具体来说，单词层面延续原有设想，将之前所有session一视同仁，利用self.history进行初始化；\n",
    "            而topic层面，不再利用self.history，而是引入session间的遗忘机制（需要仔细考虑衰减+停止参数设置的组合，怎样对于v02的约200个知识点是最好的）\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 n_diff, \n",
    "                 list_kp=(1, 208), \n",
    "                 n_words=(1, 2348), \n",
    "                 n_diff_w=None, \n",
    "                 valid_range=None, \n",
    "                 similarity=None, \n",
    "                 padding_value=-1, \n",
    "                 last_n=10, \n",
    "                 threshold=0.05,\n",
    "                 lr=0.5,\n",
    "                 decay_IntraSess=0.98,\n",
    "                 decay_InterSess=0.98,\n",
    "                 decay_PracInSess=0.8,\n",
    "                 hyperparams=None,\n",
    "                 nxneg=1.0,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param valid_range: tuple, 知识点掌握度取值范围，(0,1)\n",
    "        :param list_kp:     tuple, 知识点ID取值范围，   (1, n_questions)\n",
    "        :param similarity:  np.ndarray, KC相似度稀疏矩阵，(n_questions, n_questions)\n",
    "        :param padding_value: int, 填充值(未接触到的KC掌握度赋值为-1)\n",
    "        :param n_diff: int, 题目难度档位数，一般为3，即难度取值为1,2,3\n",
    "        :param n_diff_w: int, 单词难度档位数，一般为3，即难度取值为1,2,3\n",
    "        :param n_words: tuple, 单词ID取值范围，   (1, n_words)\n",
    "        :param last_n: int, 记录last_n步调整量用list，用于判断停止推题\n",
    "        :param threshold: float, 停止推题的阈值\n",
    "        learning_rate = 0.5    # 学习率\n",
    "        decay_factor_1 = 0.98  # 衰减因子1:session内/间遗忘（较弱）\n",
    "        decay_factor_2 = 0.9   # 衰减因子2:session内练习量（较强）\n",
    "        \"\"\"\n",
    "        self.n_diff = n_diff                     # 题目难度档位数，一般为3，即难度取值为1,2,3\n",
    "        if n_diff_w is None:\n",
    "            self.n_diff_w = n_diff               # 单词难度档位数，一般为3，即难度取值为1,2,3\n",
    "        if valid_range is None:\n",
    "            self.valid_range = (0, 1)            # 知识点掌握度取值范围（240919改为不再映射至分类，而是直接使用原有的0-1连续值，注意需要对应重新计算超参数）\n",
    "        else:\n",
    "            self.valid_range = valid_range\n",
    "        self.list_kp = list(range(list_kp[0], list_kp[1]+1))  # 知识点id列表（冷启动初始化用）\n",
    "        self.n_words = list(range(n_words[0], n_words[1]+1))  # 同上，单词id列表\n",
    "        if similarity is None:                   # KC相似度稀疏矩阵\n",
    "            self.similarity = None\n",
    "        else:\n",
    "            self.similarity = similarity         # (n_questions, n_questions) np.ndarray\n",
    "        self.padding_value = padding_value       # 填充值(未接触到的KC掌握度赋值为-1)\n",
    "        # self.history = {}                      # (deprecated) dict，形如 ind_kp: (n_p, avg_diff, avg_correctness)，仅在单词维度使用（记录学生答题记录的历史平均用）\n",
    "        # self.knowledge_points = {}             # (deprecated) 学生的topic/知识点掌握度\n",
    "        # self.mastery_words = {}                # (deprecated) 学生的单词掌握度\n",
    "        # self.adjustments = []                  # (deprecated) 记录last_n步调整量用list，用于判断停止推题\n",
    "        if hyperparams is None:\n",
    "            self.A, self.B, self.C = get_solution()  # 正确率-难度-掌握度 映射关系超参数，调用get_solution求解\n",
    "        else:\n",
    "            self.A, self.B, self.C = hyperparams     # 直接指定超参数\n",
    "        self.last_n = last_n\n",
    "        self.threshold = threshold\n",
    "        self.lr = lr                             # 学习率和衰减项相关\n",
    "        self.decay_IntraSess = decay_IntraSess\n",
    "        self.decay_InterSess = decay_InterSess\n",
    "        self.decay_PracInSess = decay_PracInSess\n",
    "        self.nxneg = nxneg                       # 负例的调整倍率\n",
    "\n",
    "    def map_difficulty(self, diff):\n",
    "        \"\"\"难度取值scaling至[0,1]\"\"\"\n",
    "        return diff / self.n_diff\n",
    "\n",
    "    def map_difficulty_w(self, diff):\n",
    "        \"\"\"难度取值scaling至[0,1]\"\"\"\n",
    "        return diff / self.n_diff_w\n",
    "\n",
    "    def init_from_history(self, user_history):\n",
    "        \"\"\"\n",
    "        单词维度：使用 self.history 初始化 self.mastery_words（取值范围[0,1]）\n",
    "                240920: 分离reset功能；对于未接触到的单词掌握度padding为-1\n",
    "        \"\"\"\n",
    "        mastery_words = {}\n",
    "        if len(user_history) > 0:                                                                                  # 若有历史记录，则调用映射公式计算得到的单词掌握度作为初始化结果\n",
    "            for ind_kp, (n_p, avg_diff, avg_corr) in user_history.items():\n",
    "                mastery_words[ind_kp] = ((self.valid_range[1] - self.valid_range[0])/self.A) * (avg_corr - self.C + self.B * (self.map_difficulty(avg_diff))**2)\n",
    "                mastery_words[ind_kp] = max(self.valid_range[0], min(mastery_words[ind_kp], self.valid_range[1]))  # clip（单词和topic的掌握度取值范围统一为0-1）\n",
    "                for _elem in self.n_words:                                                                         # 根据self.n_words，将所有不存在历史记录的单词统一初始化为padding值(-1)\n",
    "                    if _elem not in mastery_words.keys():\n",
    "                        # mastery_words[_elem] = 0.5 * (self.valid_range[1] + self.valid_range[0])                 # 初始化为取值范围中位数(0.5)\n",
    "                        mastery_words[_elem] = self.padding_value                                                  # 初始化为padding值(-1)\n",
    "        elif self.n_words is not None:                                                                             # 若无历史记录，则根据单词表统一初始化为-1\n",
    "            for _elem in self.n_words:\n",
    "                # mastery_words[_elem] = (self.valid_range[1] + self.valid_range[0]) / 2                           # 初始化为取值范围中位数(0.5)\n",
    "                mastery_words[_elem] = self.padding_value                                                          # 初始化为padding值(-1)\n",
    "\n",
    "        return mastery_words\n",
    "\n",
    "    def init_from_padding_value(self):\n",
    "        \"\"\"\n",
    "        topic/知识点维度：使用填充值初始化 self.knowledge_points（注意更新时需要先reset为有效取值范围的中位数）\n",
    "        \"\"\"\n",
    "        mastery_topic = {}\n",
    "        for _elem in self.list_kp:                       # 遍历整个topic/知识点表\n",
    "            mastery_topic[_elem] = self.padding_value    # 初始化为padding值(-1)\n",
    "        return mastery_topic\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_data(data_in):\n",
    "        \"\"\"\n",
    "        预处理接口数据（统一进行，不在外部流程中单独调用，注意避免重复操作，包含topic & 单词维度，\"old_p\" & \"new_p\"）\n",
    "        \"\"\"\n",
    "        # \"old_p\"\n",
    "        pid = []\n",
    "        q = []    # topic\n",
    "        w_q = []  # topic权重\n",
    "        w = []    # 单词\n",
    "        w_w = []  # 单词权重\n",
    "        s = []\n",
    "        diff = []\n",
    "        sess = []\n",
    "        # 逐条遍历历史做题序列中的做题记录（dict）\n",
    "        _data = data_in[\"old_p\"]\n",
    "        for _dict in _data:                # append 至对应list的元素及其属性\n",
    "            pid.append(_dict[\"pid\"])       # 题目id                 int\n",
    "            q.append(_dict[\"q\"])           # 题目涉及的topic/知识点   list of int\n",
    "            w.append(_dict[\"w\"])           # 题目涉及的单词           list of int\n",
    "            w_q.append(_dict[\"q_w\"])       # 题目涉及的知识点权重      list of float\n",
    "            w_w.append(_dict[\"w_w\"])       # 题目涉及的知识点权重      list of float\n",
    "            s.append(_dict[\"s\"])           # 答题结果                int\n",
    "            diff.append(_dict[\"diff\"])     # 题目难度                int\n",
    "            sess.append(_dict[\"session\"])  # 题目所属session         int\n",
    "\n",
    "        # \"new_p\"\n",
    "        if \"new_p\" in data_in.keys():\n",
    "            new_pid = []\n",
    "            new_q = []    # topic\n",
    "            new_w_q = []  # topic权重\n",
    "            new_w = []    # 单词\n",
    "            new_w_w = []  # 单词权重\n",
    "            # new_s = []  # 待预报序列无答题结果和session两项属性\n",
    "            new_diff = []\n",
    "            # new_sess = []\n",
    "            # 逐条遍历历史做题序列中的做题记录（dict）\n",
    "            _data = data_in[\"new_p\"]\n",
    "            for _dict in _data:                    # append 至对应list的元素及其属性\n",
    "                new_pid.append(_dict[\"pid\"])       # 题目id                 int\n",
    "                new_q.append(_dict[\"q\"])           # 题目涉及的topic/知识点   list of int\n",
    "                new_w.append(_dict[\"w\"])           # 题目涉及的单词           list of int\n",
    "                new_w_q.append(_dict[\"q_w\"])       # 题目涉及的知识点权重      list of float\n",
    "                new_w_w.append(_dict[\"w_w\"])       # 题目涉及的知识点权重      list of float\n",
    "                # new_s.append(_dict[\"s\"])           # 答题结果                int\n",
    "                new_diff.append(_dict[\"diff\"])     # 题目难度                int\n",
    "                # new_sess.append(_dict[\"session\"])  # 题目所属session         int\n",
    "\n",
    "            # 静态方法，不进行外部单独调用，有返回值\n",
    "            return pid, q, w_q, w, w_w, s, diff, sess, (new_pid, new_q, new_w_q, new_w, new_w_w, new_diff)\n",
    "        else:\n",
    "            return pid, q, w_q, w, w_w, s, diff, sess, None\n",
    "\n",
    "    @staticmethod\n",
    "    def split_sessions(pid, q, w_q, w, w_w, s, diff, sess):\n",
    "        \"\"\"\n",
    "        接续preprocess_data，拆分session（topic/单词维度通用，仅针对\"old_p\"数据）\n",
    "        返回list形式，每个元素为一个session的数据dict\n",
    "        \"\"\"\n",
    "        # topic维度，需要拆分所有session数据，因此建立session id的list（按json序列先后顺序）\n",
    "        session_ids = []\n",
    "        for _sess in sess:\n",
    "            if _sess not in session_ids:\n",
    "                session_ids.append(_sess)\n",
    "        # 初始化一个字典，用于存储分割后的子列表\n",
    "        split_data = defaultdict(lambda: {'pid': [], 'q': [], 'w_q':[], 'w': [], 'w_w': [], 's': [], 'diff': [], 'sess': []})\n",
    "        # 遍历数据，并根据session id进行分割，append至对应session的list\n",
    "        for _sess, _pid, _q, _wq, _w, _ww, _s, _diff in zip(sess, pid, q, w_q, w, w_w, s, diff):\n",
    "            split_data[_sess]['pid'].append(_pid)\n",
    "            split_data[_sess]['q'].append(_q)\n",
    "            split_data[_sess]['w_q'].append(_wq)\n",
    "            split_data[_sess]['w'].append(_w)\n",
    "            split_data[_sess]['w_w'].append(_ww)\n",
    "            split_data[_sess]['s'].append(_s)\n",
    "            split_data[_sess]['diff'].append(_diff)\n",
    "            split_data[_sess]['sess'].append(_sess)\n",
    "\n",
    "        result = []  # 将字典转换为列表（按照json序列的先后顺序）\n",
    "        for _sess in session_ids:\n",
    "            result.append(split_data[_sess])\n",
    "        return result  # list of dicts（由每个session数据字典组成的list）\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_history(data_in):\n",
    "        \"\"\"\n",
    "        （单词维度）根据历史session数据，计算history\n",
    "        遍历非当前session的所有数据，按照word建立历史统计，内容包含 ind_word: (n_p, avg_diff, avg_correctness)\n",
    "        :return: dict\n",
    "        \"\"\"\n",
    "        user_history = {}  # 初始化输出\n",
    "        # 遍历session\n",
    "        for ind_sess, current_session in enumerate(data_in):  # 解析数据：当前（遍历）session\n",
    "            # pids = current_session['pid']       # 题目id序列\n",
    "            # qs = current_session['q']           # topic维度\n",
    "            # w_qs = current_session['w_q']       # 权重序列\n",
    "            ws = current_session['w']           # 单词维度\n",
    "            # w_ws = current_session['w_w']       # 权重序列\n",
    "            ss = current_session['s']           # 答题结果序列\n",
    "            diffs = current_session['diff']     # 题目难度序列\n",
    "            # sessions = current_session['sess']  # 题目session序列\n",
    "\n",
    "            # 遍历当前session题目序列\n",
    "            for _i, (w, s, diff) in enumerate(zip(ws, ss, diffs)):\n",
    "                for _ii, _w in enumerate(w):\n",
    "                    # 遍历题目涉及的所有单词\n",
    "                    if _w not in user_history.keys():                   # 若第一次做涉及单词_w的题\n",
    "                        user_history[_w] = (1, diff, s)                 # 原始diff，未scaling\n",
    "                    else:                                               # 否则running更新user_history[_w]\n",
    "                        n_p, avg_diff, avg_corr = user_history[_w]      # 读取\n",
    "                        n_p += 1                                        # 更新元组内容\n",
    "                        avg_diff = (avg_diff * (n_p - 1) + diff) / n_p  # n_p已更新，因此-1\n",
    "                        avg_corr = (avg_corr * (n_p - 1) + s) / n_p\n",
    "                        user_history[_w] = (n_p, avg_diff, avg_corr)    # 覆盖\n",
    "\n",
    "        return user_history\n",
    "\n",
    "    def update_mastery_words(self, data_in):  # , pids, qs, ss, diffs, sessions, weights=None, last_n=10, threshold=0.05\n",
    "        \"\"\"\n",
    "        单词维度：传入历史+当前session做题序列，更新并返回知识点掌握度\n",
    "        （deprecated，同时判断是否满足停止条件）\n",
    "        :param data_in: list of dict（self.split_sessions的输出），每个元素包含：\n",
    "             pid: list of int, 题目id序列\n",
    "             q: list of list,  知识点序列\n",
    "             w_q: list of list, 知识点权重序列\n",
    "             w: list of list,  单词序列\n",
    "             w_w: list of list, 单词权重序列\n",
    "             s: list of int,   答题结果序列\n",
    "             diff: list of int, 题目难度序列\n",
    "             sess: list of int, 题目session序列\n",
    "        :return: dict, 单词掌握度，形如 {1:0.3,2:0.5,3:0.1,4:-1,5:0.2...}\n",
    "        \"\"\"\n",
    "        # （1）解析输入数据\n",
    "        history = self.calc_history(data_in[:-1])        # 使用历史session数据计算user_history dict，形如 ind_kp: (n_p, avg_diff, avg_correctness)\n",
    "        mastery_words = self.init_from_history(history)  # 初始化学生的单词掌握度 dict（不再使用self.knowledge_points等属性）\n",
    "\n",
    "        current_session = data_in[-1]                  # dict，当前session\n",
    "        pids = current_session['pid']                  # 题目id序列\n",
    "        # qs = current_session['q']                    # topic维度\n",
    "        # w_qs = current_session['w_q']                # 权重序列\n",
    "        ws = current_session['w']                      # 单词维度\n",
    "        w_ws = current_session['w_w']                  # 权重序列\n",
    "        ss = current_session['s']                      # 答题结果序列\n",
    "        diffs = current_session['diff']                # 题目难度序列\n",
    "        sessions = current_session['sess']             # 题目session序列\n",
    "\n",
    "        # （2）参数设置\n",
    "        learning_rate = self.lr                        # 学习率\n",
    "        decay_factor_1 = self.decay_IntraSess          # 衰减因子1:session内遗忘（较弱）\n",
    "        decay_factor_2 = self.decay_PracInSess         # 衰减因子2:session内练习量（较强）\n",
    "        n_ = len(pids)                                 # 当前session的题目序列长度\n",
    "        kp_count = {}                                  # 记录当前session已做过的涉及某知识点的题数，如 1: 3\n",
    "\n",
    "        # （3）遍历当前session题目序列\n",
    "        for _i, (pid, w, s, diff, weight) in enumerate(zip(pids, ws, ss, diffs, w_ws)):\n",
    "            diff_norm = self.map_difficulty_w(diff)    # 难度取值scaling至[0,1]\n",
    "            # 遍历当前题目涉及的单词\n",
    "            for _ii, k_id in enumerate(w):\n",
    "                if mastery_words[k_id] == self.padding_value:  # 对于无历史记录的单词，初始化掌握度为取值范围中位数 (0.5)\n",
    "                    mastery_words[k_id] = 0.5 * (self.valid_range[1] + self.valid_range[0])\n",
    "                \"\"\"\n",
    "                掌握度调整：考虑时间衰减和题目难度\n",
    "                调整量 = 学习率 * \n",
    "                        指数形式的时间衰减（遗忘，如果是题目在材料中的顺序的话比较合理）*\n",
    "                        题目难度（越难调整越多，这个不尽然，难题作对了奖励多，但做错了的话惩罚应该相对温和，反而是容易的题做错了惩罚大，作对了奖励小）\n",
    "                        从历史统计or当前session中获取当前kp的历史做题数，用于调整量的指数衰减（以使其稳定）\n",
    "                \"\"\"\n",
    "                # （直接调整）当前session已做过的涉及该单词的题数\n",
    "                if k_id in kp_count.keys():\n",
    "                    n_sess = kp_count[k_id]\n",
    "                    kp_count[k_id] += 1\n",
    "                else:\n",
    "                    n_sess = 0\n",
    "                    kp_count[k_id] = 1\n",
    "                \"\"\"\n",
    "                # 两部分指数衰减，一是距做完当前遍历题目&知识点已过去多少道题（session内遗忘），二是对当前遍历知识点的历史做题数\n",
    "                # adjustment = learning_rate * (decay_factor_1 ** (n_-_i)) * (decay_factor_2 ** n_hist)  # 调整量\n",
    "                # adjustment = learning_rate * (decay_factor_2 ** n_hist)                                # (deprecated) 调整量(历史总做题数衰减)\n",
    "                # adjustment = learning_rate * (decay_factor_2 ** n_sess)                                # 调整量（只考虑当前session内做题量的衰减）\n",
    "                \"\"\"\n",
    "                adjustment = learning_rate * (decay_factor_1 ** (n_-_i)) * (decay_factor_2 ** n_sess)    # 调整量（session内遗忘+做题量衰减）\n",
    "                adjustment = adjustment * weight[_ii] * len(w)                                           # 单词对于题目的相对重要性加权（denorm）\n",
    "                if s == 1:                                                                               # 不同情况下的调整策略\n",
    "                    adjustment = adjustment * diff_norm                                                  # 答对(容易题奖励小，难题奖励大)\n",
    "                else:\n",
    "                    adjustment = -1 * adjustment * ((1/self.n_diff) + 1-diff_norm)                       # 答错(容易题惩罚大，难题惩罚小)\n",
    "\n",
    "                mastery_words[k_id] += adjustment                                                              # 执行直接调整\n",
    "                mastery_words[k_id] = max(self.valid_range[0], min(mastery_words[k_id], self.valid_range[1]))  # clip\n",
    "\n",
    "        return mastery_words\n",
    "\n",
    "    def update_mastery_topics(self, data_in):\n",
    "        \"\"\"\n",
    "        topic维度：传入历史+当前session做题序列，更新并返回知识点掌握度，同时判断是否满足停止条件\n",
    "        :param data_in: list of dict（self.split_sessions的输出），每个元素包含：\n",
    "             pid: list of int, 题目id序列\n",
    "             q: list of list,  知识点序列\n",
    "             w_q: list of list, 知识点权重序列\n",
    "             w: list of list,  单词序列\n",
    "             w_w: list of list, 单词权重序列\n",
    "             s: list of int,   答题结果序列\n",
    "             diff: list of int, 题目难度序列\n",
    "             sess: list of int, 题目session序列\n",
    "        :return: dict, topic掌握度，形如 {1:0.3,2:0.5,3:0.1,4:-1,5:0.2...}\n",
    "        \"\"\"\n",
    "        # （0）参数设置\n",
    "        learning_rate = self.lr                         # 学习率\n",
    "        decay_factor_1 = self.decay_InterSess           # 衰减因子1:session间遗忘（较弱）\n",
    "        decay_factor_2 = self.decay_PracInSess          # 衰减因子2:session内练习量（较强）\n",
    "\n",
    "        # （1）初始化输出\n",
    "        mastery_topic = self.init_from_padding_value()  # 初始化topic掌握度dict（不再使用self.knowledge_points等属性）\n",
    "        avg_adj = 0.0                                   # 初始化平均调整量\n",
    "\n",
    "        # （2，topic维度）遍历session\n",
    "        for ind_sess, current_session in enumerate(data_in):  # 解析数据：当前（遍历）session\n",
    "            pids = current_session['pid']       # 题目id序列\n",
    "            qs = current_session['q']           # topic维度\n",
    "            w_qs = current_session['w_q']       # 权重序列\n",
    "            # ws = current_session['w']         # 单词维度\n",
    "            # w_ws = current_session['w_w']     # 权重序列\n",
    "            ss = current_session['s']           # 答题结果序列\n",
    "            diffs = current_session['diff']     # 题目难度序列\n",
    "            sessions = current_session['sess']  # 题目session序列\n",
    "\n",
    "            list_adjustments = []               # 初始化记录当前session调整量list，用于判断是否停止推题\n",
    "            n_ = len(pids)                      # 当前session题目序列长度\n",
    "            kp_count = {}                       # 记录当前遍历session已做过的涉及某知识点的题数，如'1': 3\n",
    "\n",
    "            # （3）遍历当前session题目序列\n",
    "            for _i, (pid, q, s, diff, weight) in enumerate(zip(pids, qs, ss, diffs, w_qs)):\n",
    "                _list_adj = []                         # 初始化记录当前题目的调整量的子list（append至list_adjustments）\n",
    "                diff_norm = self.map_difficulty(diff)  # 难度取值scaling至[0,1]\n",
    "                # 遍历当前题目涉及的topic/知识点\n",
    "                for _ii, k_id in enumerate(q):\n",
    "                    if mastery_topic[k_id] == self.padding_value:  # 对于无做题记录的topic，初始化掌握度为取值范围中位数（0.5）\n",
    "                        mastery_topic[k_id] = 0.5 * (self.valid_range[1] + self.valid_range[0])\n",
    "                    \"\"\"\n",
    "                    掌握度调整：考虑时间衰减和题目难度\n",
    "                    调整量 = 学习率 * \n",
    "                            指数形式的时间衰减（遗忘，如果是题目在材料中的顺序的话比较合理）*\n",
    "                            题目难度（越难调整越多，这个不尽然，难题作对了奖励多，但做错了的话惩罚应该相对温和，反而是容易的题做错了惩罚大，作对了奖励小）\n",
    "                            从历史统计or当前session中获取当前kp的历史做题数，用于调整量的指数衰减（以使其稳定）\n",
    "                    \"\"\"\n",
    "                    # 当前session内已做过的涉及该topic的题数\n",
    "                    if k_id in kp_count.keys():\n",
    "                        n_sess = kp_count[k_id]\n",
    "                        kp_count[k_id] += 1\n",
    "                    else:\n",
    "                        n_sess = 0\n",
    "                        kp_count[k_id] = 1\n",
    "                    \"\"\"\n",
    "                    # 两部分指数衰减，一是距做完当前遍历题目&知识点已过去多少道题（session内遗忘），二是对当前遍历知识点的历史做题数\n",
    "                    # adjustment = learning_rate * (decay_factor_1 ** (n_-_i)) * (decay_factor_2 ** n_hist)  # 调整量\n",
    "                    # adjustment = learning_rate * (decay_factor_2 ** n_hist)                                # 调整量(历史总做题数衰减)\n",
    "                    # adjustment = learning_rate * (decay_factor_2 ** n_sess)                                # 调整量（只考虑当前session内做题量的衰减）\n",
    "                    # adjustment = learning_rate * (decay_factor_1 ** (len(data_in)-1-ind_sess))             # 调整量（session间遗忘）\n",
    "                    \"\"\"\n",
    "                    adjustment = learning_rate * (decay_factor_1 ** (len(data_in)-1-ind_sess))  * (decay_factor_2 ** n_sess)  # 调整量（session间遗忘+session内做题量衰减）\n",
    "                    adjustment = adjustment * weight[_ii] * len(q)                                           # topic对于题目的相对重要性加权（denorm）\n",
    "                    if s == 1:                                                                               # 不同情况下的调整策略\n",
    "                        adjustment = adjustment * diff_norm                                                  # 答对(容易题奖励小，难题奖励大)\n",
    "                    else:                                                                                    # 240930: 新增答错的调整倍率\n",
    "                        adjustment = -1 * adjustment * ((1 / self.n_diff) + 1 - diff_norm) * self.nxneg      # 答错(容易题惩罚大，难题惩罚小)\n",
    "\n",
    "                    mastery_topic[k_id] += adjustment                                                              # 执行直接调整\n",
    "                    mastery_topic[k_id] = max(self.valid_range[0], min(mastery_topic[k_id], self.valid_range[1]))  # clip\n",
    "\n",
    "                    \"\"\"基于相似度的邻域调整\"\"\"\n",
    "                    if self.similarity is not None:\n",
    "                        for _ind, _val in enumerate(self.similarity[:, int(k_id)]):    # 取相似度矩阵的对应列进行遍历\n",
    "                            if _val > 0:                                               # 相似度矩阵对角线元素均为0——不涉及直接调整\n",
    "                                mastery_topic[_ind] += adjustment * _val               # 以_val作为倍率，执行间接调整\n",
    "                                mastery_topic[_ind] = max(self.valid_range[0],         # clip\n",
    "                                                          min(mastery_topic[_ind], self.valid_range[1]))\n",
    "\n",
    "                    _list_adj.append(adjustment)                                       # 记录当前遍历题目涉及的各个知识点（掌握度）的直接调整量至 _list_adj\n",
    "\n",
    "                # learning_rate *= decay_factor                                        # 设置session内学习率衰减（序列不长or知识点覆盖度不高的情况下不需要）\n",
    "                list_adjustments.append(_list_adj)                                     # 将当前题目的直接调整量（长度为题目涉及topic数的list）整合至list_adjustments\n",
    "\n",
    "            # （4）判断是否满足停止条件（对于最后一个session）\n",
    "            if (n_ > self.last_n) and (ind_sess == len(data_in) - 1):    # 当前session是最后一个session\n",
    "                list_adjustments = list_adjustments[-self.last_n:]       # 取最后last_n步的调整量\n",
    "                list_adjustments = sum(list_adjustments, [])             # flatten\n",
    "                list_adjustments = [abs(_x) for _x in list_adjustments]  # 取绝对值\n",
    "                avg_adj = np.mean(list_adjustments)                      # 计算最后last_n道题的平均调整量\n",
    "\n",
    "            # 打印时：按照值的大小升序排列\n",
    "            # kp_count_sorted = dict(sorted(kp_count.items(), key=lambda item: item[1]))\n",
    "            # print(kp_count_sorted)  # check\n",
    "\n",
    "        return mastery_topic, avg_adj                            # 返回掌握度和平均调整量\n",
    "\n",
    "    def predict_knowledge_mastery(self, data_in, topic_valid=None, figure=False, reduce=True, calc_mae=True):\n",
    "        \"\"\"\n",
    "        （topic & 单词维度）预测学生对单词的掌握情况，基本流程如下：\n",
    "            调用self.preprocess_data_w(data_in) 聚合接口输入；\n",
    "            调用self.split_sessions_w(pid, w, w_w, s, diff, sess) 拆分session；——————————————————需要考虑一下history为空的情况下是否有问题！！！\n",
    "            调用self.update_mastery_topics 传入拆分后的session数据，更新topic掌握度 & 近期平均变化量；\n",
    "            调用self.update_mastery_words 传入拆分后的session数据，更新words掌握度；\n",
    "            组装为输出接口格式（\"data\"项），形如：\n",
    "            {\n",
    "                \"code\": 0,\n",
    "                \"data\": {\n",
    "                    \"abilities\": [0.3,0.5,0,1,0.8,...],//每个知识点对应的掌握情况 0-1\n",
    "                    \"is_stable\": 0 or 1,\n",
    "                    \"w_abilities\":[0.3,0.5,0,1,0.8,...], //每个单词对应的掌握情况 0-1\n",
    "                    \"ability_change\": 3.45 //如果is_stable是false，不触发计算ability_change，默认为0，为True才计算，代表 最后一个session对应的能力与上一个session的能力变化\n",
    "                \"msg\": \"success\",\n",
    "                \"trace_id\": \"e363a894900e869d489470c22bfe1096\"\n",
    "            }\n",
    "        \"\"\"\n",
    "        # 预处理接口数据\n",
    "        pid, q, w_q, w, w_w, s, diff, sess, new_data = self.preprocess_data(data_in)\n",
    "        # 拆分session\n",
    "        session_data = self.split_sessions(pid, q, w_q, w, w_w, s, diff, sess)  # list of dicts（由每个session数据字典组成的list）\n",
    "        # 更新topic/知识点掌握度\n",
    "        updated_knowledge_points, avg_adj = self.update_mastery_topics(session_data)\n",
    "        # 更新words/单词掌握度\n",
    "        updated_mastery_words = self.update_mastery_words(session_data)\n",
    "        # 组装为输出接口格式\n",
    "        _result = {\n",
    "            \"abilities\": updated_knowledge_points,       # {1:0.3,2:0.5,3:0.1,4:-1,5:0.2...}, //每个知识点对应的掌握情况 0-1\n",
    "            \"is_stable\": int(avg_adj < self.threshold),  # 0 or 1，结果是否稳定并停止推题（json不能出现bool）\n",
    "            \"w_abilities\": updated_mastery_words,        # {1:0.3,2:0.5,3:0.1,4:-1,5:0.2...}, //每个单词对应的掌握情况 0-1\n",
    "            \"ability_change\": avg_adj,                   # deprecated（topic维度）\n",
    "        }\n",
    "\n",
    "        # 240926:计算mae & 画图（241009新增参数，防止某些情况下无法计算mae）\n",
    "        if calc_mae:\n",
    "            mae = self.calc_pred_error(_result, data_in, topic_valid, figure=figure, reduce=reduce)\n",
    "        else:\n",
    "            mae = None\n",
    "\n",
    "        return _result, mae\n",
    "\n",
    "    def predict_correct_rate(self, interface_mastery, exercise_in):\n",
    "        \"\"\"\n",
    "        根据新题目和掌握情况预测答对概率.\n",
    "        :param exercise_in: tuple, 待预测新题目的信息;                     1\n",
    "        :param interface_mastery: self.predict_knowledge_mastery 的输出;\n",
    "        :return: float 答对概率.\n",
    "        \"\"\"\n",
    "        q, w, diff, weights_q, weights_w = exercise_in    # 解析待预测题目\n",
    "        topic_mastery = interface_mastery['abilities']    # {1:0.3,2:0.5,3:0.1,4:-1,5:0.2...}, //每个知识点对应的掌握情况 0-1\n",
    "        words_mastery = interface_mastery['w_abilities']  # {1:0.3,2:0.5,3:0.1,4:-1,5:0.2...}, //每个单词对应的掌握情况 0-1\n",
    "\n",
    "        # 计算平均掌握程度\n",
    "        avg_mastery_topic = 0\n",
    "        for _ind, t_id in enumerate(q):\n",
    "            temp = topic_mastery.get(t_id, 0)             # 获取掌握度，默认值为0\n",
    "            if temp == self.padding_value:                # 未接触过的单词(-1)默认未掌握\n",
    "                temp = 0\n",
    "            avg_mastery_topic += temp * weights_q[_ind]   # 加权求和\n",
    "        avg_mastery_words = 0\n",
    "        for _ind, w_id in enumerate(w):\n",
    "            temp = words_mastery.get(w_id, 0)             # 获取掌握度，默认值为0\n",
    "            if temp == self.padding_value:                # 未接触过的单词(-1)默认未掌握\n",
    "                temp = 0\n",
    "            avg_mastery_words += temp * weights_w[_ind]   # 加权求和\n",
    "        # topic和单词权重对半分，若单词为空，则topic权重为1\n",
    "        if len(w) > 0:\n",
    "            avg_knowledge_level = 0.5 * (avg_mastery_topic + avg_mastery_words)\n",
    "        else:\n",
    "            avg_knowledge_level = avg_mastery_topic\n",
    "\n",
    "        # 逆映射至作答正确率：对应于init_from_history中映射公式\n",
    "        corr_rate = avg_knowledge_level / ((self.valid_range[1] - self.valid_range[0])/self.A)\n",
    "        corr_rate = corr_rate + self.C - self.B * (self.map_difficulty(diff))**2\n",
    "\n",
    "        return max(0, min(corr_rate, 1))  # clip and return\n",
    "\n",
    "    def predict_answer_accuracy(self, data_in):\n",
    "        \"\"\"\n",
    "        根据topic&单词掌握情况预测新题目的答对概率，于batch_process或其他外部脚本如server.py中调用\n",
    "        :param data_in: dict, 包括当前session的历史做题序列和未做题目序列\n",
    "        数据接口形如\n",
    "        {\n",
    "            \"old_p\":[  //一个学生的所有做题序列，按时间排序\n",
    "                {\n",
    "                    \"pid\": 50,              //题目 id\n",
    "                    \"q\": [5,18,11],         //题目对应的知识点id,一道题目包含多个知识点\n",
    "                    \"q_w\": [0.3,0.2,0.5],   //每个知识点对应的权重\n",
    "                    \"w\": [1,4,8],           //题目对应的单词 id\n",
    "                    \"w_w\":[2,3,1],          //每个单词在题目中的重要性\n",
    "                    \"s\":1,                  //对应是否答对，1对正确，0为错误，\n",
    "                    \"diff\":2,               //每道题对应的难度\n",
    "                    \"session\": 1            //题目对应的session id\n",
    "                },\n",
    "                {\n",
    "                    \"pid\": 34,\n",
    "                    \"q\": [16,13,11],\n",
    "                    \"q_w\": [0.3,0.2,0.5],\n",
    "                    \"w\": [1,4,8],\n",
    "                    \"w_w\":[2,3,1],\n",
    "                    \"s\":1,\n",
    "                    \"diff\":2,\n",
    "                    \"session\":2\n",
    "                },\n",
    "                ...\n",
    "            ],\n",
    "            \"new_p\":[  //需要预测的新题\n",
    "                {\n",
    "                    \"pid\": 30,\n",
    "                    \"q\": [5,18,11],\n",
    "                    \"q_w\": [0.3,0.2,0.5],\n",
    "                    \"w\": [1,4,8],\n",
    "                    \"w_w\":[2,3,1],\n",
    "                    \"diff\":2,\n",
    "                },\n",
    "                {\n",
    "                    \"pid\": 28,\n",
    "                    \"q\": [5,18,11],\n",
    "                    \"q_w\": [0.3,0.2,0.5],\n",
    "                    \"w\": [1,4,8],\n",
    "                    \"w_w\":[2,3,1],\n",
    "                    \"diff\":2,\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        }\n",
    "        输出接口形如\n",
    "        {\n",
    "            \"code\": 0,\n",
    "            \"data\": {\n",
    "                30: 0.2985307652665116,\n",
    "                28: 0.1256192803111844,\n",
    "                46: 0.1467879400252059,\n",
    "                3: 0.3068059584165894\n",
    "               ] //学生对每个新题的做题概率\n",
    "            },\n",
    "            \"msg\": \"success\",\n",
    "            \"trace_id\": \"285d3c22db106c59098c8e6734834604\"\n",
    "        }\n",
    "        \"\"\"\n",
    "        # 调用获取能力接口\n",
    "        interface_mastery, _ = self.predict_knowledge_mastery(data_in)\n",
    "        # 待预报题目序列\n",
    "        _, _, _, _, _, _, _, _, new_data = self.preprocess_data(data_in)\n",
    "        new_pid, new_q, new_weights_q, new_w, new_weights_w, new_diff = new_data\n",
    "        # 调用self.predict_correct_rate，逐题预测答对概率\n",
    "        answer_prob = {}                            # 初始化\n",
    "        for i in range(len(new_pid)):               # 逐题遍历\n",
    "            new_exercise = new_q[i], new_w[i], new_diff[i], new_weights_q[i], new_weights_w[i]\n",
    "            result_ = self.predict_correct_rate(interface_mastery,\n",
    "                                                new_exercise\n",
    "                                                )   # 返回答对概率，float\n",
    "            answer_prob[new_pid[i]] = result_       # 以new_pid（int）为key，答对概率（float）为value\n",
    "\n",
    "        return answer_prob\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(predictions, targets):\n",
    "        \"\"\"使用均方误差（MSE）作为损失函数\"\"\"\n",
    "        return np.mean((np.array(predictions) - np.array(targets)) ** 2)\n",
    "\n",
    "    '''def batch_process(self, data_batches):\n",
    "        \"\"\"(deprecated)对于list形式的batch输入，遍历并调用predict_answer_accuracy，统计平均损失\"\"\"\n",
    "        total_loss = 0\n",
    "        total_len = 0\n",
    "        # 遍历batch中的每个session/序列\n",
    "        for _data in data_batches:\n",
    "            predictions_dict = self.predict_answer_accuracy(_data)           # 返回字典，key为new_pid，value为答对概率\n",
    "            new_pid = _data.get('new_pid')                                   # [30, 28, 45, 3]\n",
    "            predictions_list = [predictions_dict[_key] for _key in new_pid]  # 按new_pid的顺序，获取答对概率\n",
    "            targets = _data.get('new_s')                                     # 真实的答对情况\n",
    "            loss = self.calculate_loss(predictions_list, targets)            # 计算MSE损失\n",
    "            total_loss += loss                                               # 总损失\n",
    "            total_len += len(new_pid)                                        # 题目总数\n",
    "        return total_loss / total_len                                        # 返回平均损失'''\n",
    "    \n",
    "    def calc_pred_error(self, predictions, targets, topic_valid=None, figure=False, reduce=True):\n",
    "        \"\"\"\n",
    "        计算topic掌握度的预测误差\n",
    "        \"\"\"\n",
    "        pred = predictions[\"abilities\"]             # dict\n",
    "        true = targets[\"student\"][\"topic_mastery\"]  # dict\n",
    "\n",
    "        # 若存在有效kp list，则过滤知识点\n",
    "        if topic_valid is not None and isinstance(topic_valid, list):\n",
    "            pred = {k: v for k, v in pred.items() if int(k) in topic_valid}\n",
    "            true = {k: v for k, v in true.items() if int(k) in topic_valid}\n",
    "        \n",
    "        # 若存在能力字典，则：（240930）\n",
    "        if topic_valid is not None and isinstance(topic_valid, dict):\n",
    "            # 对齐格式 & 过滤统计估计中值为-1的item\n",
    "            true = {k: v for k, v in topic_valid.items() if v != self.padding_value}  # 键为str\n",
    "            pred = {k: v for k, v in pred.items() if str(k) in true.keys()}           # 注意与true对齐keys\n",
    "\n",
    "        # 遍历预报字典中的键值对（键为int），剔除未涉及topic（-1），同时匹配真实值中的能力(注意由于是读取自json，键为str)\n",
    "        _p = np.ones(self.list_kp[-1]+1) * self.padding_value\n",
    "        _t = np.ones(self.list_kp[-1]+1) * self.padding_value\n",
    "        for _key, _val in pred.items():\n",
    "            if _val == self.padding_value:\n",
    "                continue\n",
    "            _p[int(_key)] = float(_val)             # 确保一下数据格式正确\n",
    "            _t[int(_key)] = float(true[str(_key)])\n",
    "        \n",
    "        # 计算difference，并以pdf的形式统计出来\n",
    "        ind_filter = np.where(_p==self.padding_value)[0]\n",
    "        pred = np.delete(_p, ind_filter)\n",
    "        true = np.delete(_t, ind_filter)\n",
    "        difference = pred - true\n",
    "        mae = np.mean(np.abs(difference))  # MAE\n",
    "\n",
    "        # print(difference.shape)  # check\n",
    "        # print(difference)\n",
    "\n",
    "        if figure:\n",
    "            # 调用函数绘制pdf图\n",
    "            # plot_pdf(difference)\n",
    "\n",
    "            text_in = \"MAE={}, lr={}, decay={}\".format(round(mae,2),self.lr,self.decay_PracInSess)\n",
    "            # 保存图片路径为：pics/histogram_lr_{}_decay_{}_{timestamp}.png\n",
    "            now = datetime.now()\n",
    "            now = now.strftime(\"%Y-%m-%d_%H-%M-%S.%f\")      # 默认格式：YYYY-MM-DD_HH-MM-SS.mmmmmm\n",
    "            filename = 'pics/histogram_lr_{}_dacay_{}_{}.png'.format(self.lr, self.decay_PracInSess, now)\n",
    "            plot_histogram(difference, text=text_in, file_out=filename)  # 直方图\n",
    "        \n",
    "        # 返回值\n",
    "        if reduce:\n",
    "            return mae\n",
    "        else:\n",
    "            return difference\n",
    "\n",
    "    '''def calc_acc_error(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        (未完成)计算答题正确率的预测误差\n",
    "        \"\"\"\n",
    "        pred = predictions[\"abilities\"]             # dict\n",
    "        true = targets[\"student\"][\"topic_mastery\"]  # dict\n",
    "        \n",
    "        # 遍历预报字典中的键值对（键为int），剔除未涉及topic（-1），同时匹配真实值中的能力(注意由于是读取自json，键为str)\n",
    "        _p = np.ones(self.list_kp[-1]+1) * self.padding_value\n",
    "        _t = np.ones(self.list_kp[-1]+1) * self.padding_value\n",
    "        for _key, _val in pred.items():\n",
    "            if _val == self.padding_value:\n",
    "                continue\n",
    "            _p[int(_key)] = float(_val)             # 确保一下数据格式正确\n",
    "            _t[int(_key)] = float(true[str(_key)])\n",
    "        \n",
    "        # 计算difference，并以pdf的形式统计出来\n",
    "        ind_filter = np.where(_p==self.padding_value)[0]\n",
    "        pred = np.delete(_p, ind_filter)\n",
    "        true = np.delete(_t, ind_filter)\n",
    "        difference = pred - true\n",
    "        mae = np.mean(np.abs(difference))  # MAE\n",
    "\n",
    "        # print(difference.shape)  # check\n",
    "        # print(difference)\n",
    "\n",
    "        # 调用函数绘制pdf图\n",
    "        # plot_pdf(difference)\n",
    "\n",
    "        text_in = \"MAE={}\".format(round(mae,2))\n",
    "        plot_histogram(difference, text=text_in)  # 直方图'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 调用策略，生成学生能力并保存为json接口格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定参数取值\n",
    "nxneg = 1.4\n",
    "decay_factor = 0.9\n",
    "lr = 0.5\n",
    "\n",
    "# 需要进行设置的参数\n",
    "out_dir = os.path.join(root_dir, f'{datasets}/')  # 输出文件夹\n",
    "ensure_directory_exists(out_dir)\n",
    "kp_id_start = 0\n",
    "n_kp = 93\n",
    "list_kp = (kp_id_start, kp_id_start+n_kp-1)\n",
    "padding_value = -1\n",
    "\n",
    "# 读取开源数据\n",
    "with open(os.path.join(root_dir, f'{datasets}/train.json'), \"r\", encoding='UTF-8') as f:\n",
    "    data_dbe = json.load(f)  # list of dicts\n",
    "# 读取统计估算的能力文件(统计baseline)\n",
    "with open(os.path.join(root_dir, f'{datasets}/stu_cap_stat.json'), \"r\", encoding='UTF-8') as f:\n",
    "    data_cap = json.load(f)  # dict of dicts（\"stu_id\":{\"0\":xxx,\"1\":xxx,...}）\n",
    "\n",
    "# # check\n",
    "# print(data_dbe[0].keys())                       # ['old_p', 'new_p', 'student']\n",
    "# print(data_dbe[0][\"student\"].keys())            # ['topic_mastery', 'id']\n",
    "# print(data_dbe[0][\"student\"][\"topic_mastery\"])  # {}，待填充\n",
    "# first_id = data_dbe[0][\"student\"][\"id\"]\n",
    "# print(type(first_id))                           # int\n",
    "# print(data_cap[str(first_id)].keys())           # 0-92，齐全\n",
    "# print(data_cap[str(first_id)].values())         # -1填充\n",
    "\n",
    "# 根据gridsearch直接实例化\n",
    "complete_init = ConceptPredictStrategy(n_diff=1,                # 无难度，统一为1\n",
    "                                       list_kp=list_kp,         # n_question=93（从0开始，最大为92）\n",
    "                                       n_words=(1,2),           # 无单词\n",
    "                                       n_diff_w=None,           # deprecated\n",
    "                                       valid_range=(0,1),\n",
    "                                       similarity=None,         # 无间接调整\n",
    "                                       padding_value=-1,\n",
    "                                       last_n=10,\n",
    "                                       threshold=0.05,\n",
    "                                       lr=lr,                            # 直接指定为gridsearch结果\n",
    "                                       decay_IntraSess=1.0,     # 不生效（无单词）\n",
    "                                       decay_InterSess=1.0,     # 不生效（无session）\n",
    "                                       decay_PracInSess=decay_factor,    # 直接指定\n",
    "                                       hyperparams=(2/3, 0, 0.25),       # 240930新增：手动指定超参数\n",
    "                                       nxneg=nxneg,                      # 直接指定为gridsearch结果\n",
    "                                       )\n",
    "\n",
    "# 遍历数据集中的每个学生，获取能力\n",
    "for ind, stu in enumerate(data_dbe):\n",
    "    stu_id = stu[\"student\"][\"id\"]      # int\n",
    "    # stu_cap = data_cap[str(stu_id)]  # dict，根据stu_id查询统计的能力统计估计，键为str\n",
    "    stu_cap = {}                       # 正常初始化\n",
    "    for key in range(kp_id_start, kp_id_start+n_kp):\n",
    "        stu_cap[str(key)] = padding_value\n",
    "\n",
    "    temp, _ = complete_init.predict_knowledge_mastery(stu,\n",
    "                                                      calc_mae=False,         # 241009新增：不计算mae（返回None）\n",
    "                                                      topic_valid=None,       # list（参与计算mae的所有topic）\n",
    "                                                      figure=False,           # 不画图\n",
    "                                                      reduce=False)           # 不计算平均\n",
    "    # 记录至data_dbe\n",
    "    data_dbe[ind][\"student\"][\"topic_mastery\"] = temp[\"abilities\"]             # dict\n",
    "    \n",
    "\n",
    "# 保存结果\n",
    "filename = os.path.join(root_dir, f'{datasets}/train_cap_strategy.json')\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_dbe, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 检查：绘制能力分布直方图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['old_p', 'new_p', 'student'])\n",
      "dict_keys(['topic_mastery', 'id'])\n",
      "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92'])\n",
      "dict_values([1, 0.9485272169763377, 1, 1, 1, 0.7021273195, 1, 1, 0.81742195, 0.775, 1.0, 0.030429999999999957, 0.36450125787699983, 1, 0.855, 1, 0.45, 0.775, 1, 0.45, 1.0, 0.64885369138507, 1, 1, 1.0, 0, 0.9995, 0.6414791423769549, 1, 1.0, 0, -1, -1, -1, -1, -1, -1, -1, 0.9995, 1, 1, 1, 1, 0.9995, 0.45, 0.45, 1.0, 1.0, 1, 1, 1.0, 1.0, 1, 0.9234833009003334, 0, 1, 0.4896999999999999, 1, 1, 1.0, 0.9874305000000001, 0.34435530000000003, 0.6724053000000001, 1, 1, 0.7971251199669487, 0.2894031052830002, 1, 0, 1, 0, 0, 0.37, 0, 0, 0, 1.0, 1.0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
      "<class 'int'>\n",
      "{'pid': 173, 'q': [69], 'q_w': [1.0], 'w': [], 'w_w': [], 's': 1, 'diff': 1, 'session': 1, 'corrate': -1}\n",
      "correct rate in total: 77.27%\n"
     ]
    }
   ],
   "source": [
    "# 读取上一步骤的输出\n",
    "out_dir = os.path.join(root_dir, f'{datasets}/')  # 输出文件夹\n",
    "filename = os.path.join(out_dir, \"train_cap_strategy.json\")\n",
    "with open(filename, \"r\", encoding='UTF-8') as f:\n",
    "    data_dbe = json.load(f)\n",
    "\n",
    "# check\n",
    "print(data_dbe[0].keys())                       # ['old_p', 'new_p', 'student']\n",
    "print(data_dbe[0][\"student\"].keys())            # ['topic_mastery', 'id']\n",
    "print(data_dbe[0][\"student\"][\"topic_mastery\"].keys())\n",
    "print(data_dbe[0][\"student\"][\"topic_mastery\"].values())\n",
    "first_id = data_dbe[0][\"student\"][\"id\"]\n",
    "print(type(first_id))                           # int\n",
    "print(data_dbe[0][\"old_p\"][0])\n",
    "\n",
    "capacities = []\n",
    "for stu in data_dbe:\n",
    "    temp = list(stu[\"student\"][\"topic_mastery\"].values())\n",
    "    capacities.extend(temp)\n",
    "\n",
    "# 画图(batch process中单独进行)\n",
    "text_in = \"student capacities\"\n",
    "now = datetime.now()\n",
    "now = now.strftime(\"%Y-%m-%d_%H-%M-%S.%f\")    # 默认格式：YYYY-MM-DD_HH-MM-SS.mmmmmm\n",
    "filename = 'histogram_stu_cap_strategy_DBE_KT22_{}.png'.format(now)\n",
    "plot_histogram(capacities, text=text_in, file_out=os.path.join('.', filename))  # 直方图\n",
    "\n",
    "# 顺便也检查一下整体正确率\n",
    "answer = []\n",
    "for stu in data_dbe:\n",
    "    for exercise in stu[\"old_p\"]:\n",
    "        answer.append(exercise[\"s\"])\n",
    "\n",
    "print(\"correct rate in total: {:.2f}%\".format(sum(answer)/len(answer)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 能力分类映射函数，并使用其统计类别比例，为后续加权做准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 0.45922587397241776, 2: 0.1357168177444675, 1: 0.1315305278438919, 0: 0.11321550952887359, -1: 0.16031127091034925}\n",
      "dict_keys([3, 2, 1, 0, -1])\n",
      "dict_values([0.45922587397241776, 0.1357168177444675, 0.1315305278438919, 0.11321550952887359, 0.16031127091034925])\n",
      "59241\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "def mastery_level(score):\n",
    "    if score == 1:\n",
    "        return 3   # \"精通\"\n",
    "    elif score >= 0.5:\n",
    "        return 2   # \"熟练\"\n",
    "    elif score > 0:\n",
    "        return 1   # \"模糊\"\n",
    "    elif score == 0:\n",
    "        return 0   # \"未掌握\"\n",
    "    else:\n",
    "        # return -1   # 与padding值对齐\n",
    "        return score  # -1\n",
    "    \n",
    "# 深拷贝上一步中的结果\n",
    "data_in = copy.deepcopy(capacities)\n",
    "for ind, elem in enumerate(capacities):\n",
    "    data_in[ind] = mastery_level(elem)\n",
    "\n",
    "# 统计\n",
    "category_proportion = dict(Counter(data_in))\n",
    "total_count = sum(category_proportion.values())  # 59241\n",
    "for key, val in category_proportion.items():\n",
    "    category_proportion[key] = val/total_count\n",
    "\n",
    "# check\n",
    "print(category_proportion)\n",
    "print(category_proportion.keys())\n",
    "print(category_proportion.values())\n",
    "print(total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 能力映射分类，格式转换json->txt，适配模型输入（qid的修改在这里进行！！！）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "# 输出路径\n",
    "out_path = os.path.join(root_dir, f'{datasets}/train_cap_strategy.txt')\n",
    "\n",
    "# 读取开源数据\n",
    "with open(os.path.join(root_dir, f'{datasets}/train_cap_strategy.json'), \"r\", encoding='UTF-8') as f:\n",
    "    data_dbe = json.load(f)  # list of dicts\n",
    "\n",
    "# # check\n",
    "# print(data_dbe[0].keys())                       # ['old_p', 'new_p', 'student']\n",
    "# print(data_dbe[0][\"old_p\"][0])\n",
    "# print(data_dbe[0][\"student\"].keys())            # ['topic_mastery', 'id']\n",
    "# print(data_dbe[0][\"student\"][\"topic_mastery\"])  # {}，待填充\n",
    "# first_id = data_dbe[0][\"student\"][\"id\"]\n",
    "# print(type(first_id))                           # int\n",
    "\n",
    "# 逐个学生进行遍历\n",
    "for stu in data_dbe:\n",
    "    n_pid= str(len(stu[\"old_p\"]))  # 做题序列长度\n",
    "    pids = []                      # 初始化\n",
    "    qs = []\n",
    "    ss = []\n",
    "    \n",
    "    # 逐道题遍历，提取信息\n",
    "    for dict_exercise in stu[\"old_p\"]:\n",
    "        pids.append(dict_exercise[\"pid\"])  # int\n",
    "\n",
    "        # qs.append(dict_exercise[\"q\"])    # list of lists\n",
    "        \"\"\"================================ 修改qid（对齐模型需要）==================================\"\"\"\n",
    "        temp = copy.deepcopy(dict_exercise[\"q\"])\n",
    "        temp = [x+n_kp if x==kp_id_start else x for x in temp]  # 将 id 0 替换为 93\n",
    "        qs.append(temp)                    # list of lists\n",
    "        \"\"\"======================================= done! ========================================\"\"\"\n",
    "        ss.append(dict_exercise[\"s\"])      # int\n",
    "\n",
    "    # 读取掌握度相关的信息\n",
    "    qid = stu[\"student\"][\"topic_mastery\"].keys()\n",
    "    qval = stu[\"student\"][\"topic_mastery\"].values()\n",
    "    # 映射qval到分类（对齐模型输入输出）\n",
    "    qval = [mastery_level(x) for x in qval]\n",
    "    \n",
    "    # 转为字符串格式\n",
    "    pids = \",\".join(map(str, pids))\n",
    "    qs = \",\".join([\"_\".join(map(str, x)) for x in qs])\n",
    "    ss = \",\".join(map(str, ss))\n",
    "    qid = \"_\".join(qid)\n",
    "    qval = \"_\".join(map(str, qval))\n",
    "   \n",
    "    # # check\n",
    "    # print(qs)\n",
    "    # print(qs.count(\",\"))\n",
    "    # print(pids.count(\",\"))\n",
    "    # print(qid)\n",
    "    # print(qval)\n",
    "    # print(qid.count(\"_\"))\n",
    "    # print(qval.count(\"_\"))\n",
    "\n",
    "    # 整合当前学生的信息并保存\n",
    "    stu = [n_pid, pids, qs, ss, qid, qval]\n",
    "    with open(out_path, \"a\", encoding=\"utf-8\") as file:  # 打开文件（追加模式），并逐行写入\n",
    "        for line in stu:\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3822\n"
     ]
    }
   ],
   "source": [
    "# %% 拆分训练/测试集\n",
    "import os\n",
    "\n",
    "def split_txt_file(input_file, n, output_file1, output_file2):\n",
    "    \"\"\"\n",
    "    分割txt文件为两个部分：前n行保存为一个文件，n+1行到最后一行保存为另一个文件。\n",
    "    :param input_file: 输入的txt文件路径\n",
    "    :param n: 前n行的行数\n",
    "    :param output_file1: 输出的第一个txt文件路径（保存前n行）\n",
    "    :param output_file2: 输出的第二个txt文件路径（保存n+1至最后一行）\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        print(len(lines))\n",
    "\n",
    "    with open(output_file1, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(lines[:n])\n",
    "\n",
    "    with open(output_file2, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(lines[n:])\n",
    "\n",
    "\n",
    "# 创建输出路径\n",
    "out_dir = os.path.join(root_dir, f'{datasets}/cap_strategy/')  # 输出文件夹\n",
    "ensure_directory_exists(out_dir)\n",
    "\n",
    "input_file = os.path.join(root_dir, f'{datasets}/train_cap_strategy.txt')  # 上一步骤输出\n",
    "output_file1 = os.path.join(out_dir, 'train.txt')\n",
    "output_file2 = os.path.join(out_dir, 'test.txt')\n",
    "n = 6*500  # 500 of 637\n",
    "\n",
    "split_txt_file(input_file, n, output_file1, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 241028 temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/new_pfs/liming_team/auroraX/songchentao/ketcat/data\n",
      "/mnt/new_pfs/liming_team/auroraX/songchentao/ketcat/data\n"
     ]
    }
   ],
   "source": [
    "# import math\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')   # 确保你的图表在后台生成且不显示图形窗口\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "# from scipy.sparse import lil_matrix\n",
    "\n",
    "# cur_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "from config import BASE_DIR\n",
    "cur_dir = BASE_DIR\n",
    "# root_dir = os.path.dirname(os.path.dirname(os.path.dirname(cur_dir)))  # 向上追溯三级\n",
    "root_dir = cur_dir\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "print(cur_dir)   # /mnt/new_pfs/liming_team/auroraX/songchentao/ketcat/data\n",
    "print(root_dir)  # 同上\n",
    "\n",
    "\n",
    "def ensure_directory_exists(path_in):\n",
    "    \"\"\"确保路径存在，否则递归地建立文件夹\"\"\"\n",
    "    if not os.path.exists(path_in):\n",
    "        os.makedirs(path_in)\n",
    "\n",
    "\n",
    "def plot_pdf(data_in):\n",
    "    \"\"\"\n",
    "    绘制概率密度函数图\n",
    "    :param data_in: 1d array\n",
    "    \"\"\"\n",
    "    # if data_in is None:\n",
    "    #     # 生成正态分布示例数据\n",
    "    #     np.random.seed(42)\n",
    "    #     data_in = np.random.normal(loc=0, scale=1, size=1000)\n",
    "\n",
    "    # 使用 seaborn 绘制概率密度函数图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.kdeplot(data_in, fill=True, color=\"b\")\n",
    "\n",
    "    # 添加标题和标签\n",
    "    plt.title('Probability Density Function (PDF)')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    # 显示网格\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 保存\n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%Y-%m-%d_%H:%M:%S.%f\")  # 默认格式：YYYY-MM-DD HH:MM:SS.mmmmmm\n",
    "    plt.savefig('pics/pdf_{}.png'.format(now))\n",
    "\n",
    "\n",
    "def plot_histogram(data_in=None, text=None, file_out=None):\n",
    "    \"\"\"\n",
    "    绘制直方图\n",
    "    :param data_in: 1d array\n",
    "    :param text: str or None\n",
    "    :param file_out: str or None\n",
    "    \"\"\"\n",
    "    if data_in is None:\n",
    "        # data_in = [0.0274, 0.0818, 0.1141, 0.2205, 0.4185, 0.0846, 0.02, 0.0207, 0.0123]\n",
    "        data_in = [2.504, 2.789, 3.830, 63.902, 11.380, 9.520, 6.074]\n",
    "    if text is None:\n",
    "        text = ''\n",
    "\n",
    "    # 使用 seaborn 绘制概率密度函数图\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # bins = np.linspace(-1,1,31).tolist()\n",
    "    # bins = np.linspace(-4,4,9).tolist()\n",
    "    bins = np.linspace(-3,3,7).tolist()\n",
    "\n",
    "    plt.bar(bins, data_in, color='b', alpha=0.7, edgecolor='black', width=1.0)  # bar图\n",
    "    # plt.hist(data_in, bins=bins, color='b', alpha=0.7, edgecolor='black')  # 柱状统计图\n",
    "\n",
    "    plt.xlim([-4,4])\n",
    "    plt.ylim([0,100])\n",
    "\n",
    "    # 计算文本显示位置\n",
    "    ax = plt.gca()\n",
    "    x_limits = ax.get_xlim()\n",
    "    y_limits = ax.get_ylim()\n",
    "    x_text = x_limits[1] + (x_limits[1] - x_limits[0]) * 0.05  # 超出横坐标最大值的5%\n",
    "    y_text = y_limits[1] + (y_limits[1] - y_limits[0]) * 0.1  # 超出纵坐标最大值的10%\n",
    "    plt.text(x_text, y_text, text, fontsize=12, ha='right', va='top')  # 显示文本\n",
    "\n",
    "    # 添加标题和标签\n",
    "    plt.title('Histogram')\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('%')\n",
    "\n",
    "    # 显示网格\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 保存\n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%Y-%m-%d_%H:%M:%S.%f\")  # 默认格式：YYYY-MM-DD HH:MM:SS.mmmmmm\n",
    "    if file_out is None:\n",
    "        file_out = 'histogram_{}.png'.format(now)  # 默认保存文件名\n",
    "    plt.savefig(file_out)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_histogram()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
